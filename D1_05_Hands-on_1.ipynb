{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43def0fd-3fba-4e39-a4bb-66a413b888aa",
   "metadata": {},
   "source": [
    "## Hands-On Example: Applying What You've Learned\n",
    "\n",
    "In this notebook, you will apply the concepts covered in the previous sessions, including:\n",
    "\n",
    "1. Understanding the Huggingface Ecosystem\n",
    "2. Working with Transformer models\n",
    "3. Implementing Tokenization and Embeddings\n",
    "4. Utilizing a pre-trained model for a NLP task\n",
    "\n",
    "### Objective:\n",
    "\n",
    "Fine-tune a pre-trained Transformer model (e.g., BERT) on a text classification task (sentiment analysis using the IMDB dataset). During this exercise, you will:\n",
    "- Load and preprocess the dataset.\n",
    "- Tokenize the input data.\n",
    "- Apply a pre-trained model to extract embeddings.\n",
    "- Fine-tune the model using memory-efficient techniques.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11df24af-1868-4273-9c88-10095509a798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fe31d9-ee97-4cde-829b-31328db38fb5",
   "metadata": {},
   "source": [
    "## Load and Explore the Dataset\n",
    "\n",
    "We will use the IMDB dataset for binary sentiment classification. The goal is to classify movie reviews as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a534f50-5960-4e67-ba9e-2004eda2ad99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from the IMDB dataset: {'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "# Load the IMDB dataset\n",
    "dataset = load_dataset(\"/leonardo_scratch/fast/EUHPC_D20_063/huggingface/datasets/stanfordnlp--imdb\")\n",
    "\n",
    "# Display a sample from the dataset\n",
    "print(\"Sample from the IMDB dataset:\", dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64767097-7870-481e-8059-e4eb1f56f740",
   "metadata": {},
   "source": [
    "## Tokenize the Data\n",
    "\n",
    "Use a pre-trained tokenizer to convert the text data into token IDs that the model can understand. We will use the BERT tokenizer for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e8e6c8e-9ac6-4ba3-936e-9b6d1af7ca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/leonardo_scratch/fast/EUHPC_D20_063/huggingface/models/google--bert-base-uncased\")\n",
    "\n",
    "# Define a function to tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set the format for PyTorch\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f6b225-d516-4df0-ac20-7fe8d8cc7d05",
   "metadata": {},
   "source": [
    "## Load a Pre-trained Model\n",
    "\n",
    "Now, let's load a pre-trained BERT model for sequence classification. This model will be fine-tuned on the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5e02f04-a41c-4cc3-b7da-2a484f7dc509",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /leonardo_scratch/fast/EUHPC_D20_063/huggingface/models/google--bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"/leonardo_scratch/fast/EUHPC_D20_063/huggingface/models/google--bert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc72aaf5-9218-4c68-9c69-e8e8e3f6d2a5",
   "metadata": {},
   "source": [
    "## Configure Training Arguments\n",
    "\n",
    "Set up the training arguments for fine-tuning the model. This includes the output directory, batch size, number of epochs, and logging settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c015d79-07e6-49f0-afbf-43b2afcc0419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af94393b-354a-4897-8d58-5305b2acceb4",
   "metadata": {},
   "source": [
    "## Fine-Tune the Model\n",
    "\n",
    "Use the Huggingface `Trainer` API to fine-tune the model on the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9bfa783-df99-4eff-825e-4ee54e7b5061",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-08 10:22:08,075] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-09-08 10:22:10,166] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a2c3030-6467-4f95-9fbf-18b9f8619e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 03:23, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.259100</td>\n",
       "      <td>0.304622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3125, training_loss=0.37470179908752443, metrics={'train_runtime': 203.8489, 'train_samples_per_second': 122.64, 'train_steps_per_second': 15.33, 'total_flos': 1644444096000000.0, 'train_loss': 0.37470179908752443, 'epoch': 1.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31b2c3b-00ea-4e71-80cb-c8b39c84307a",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "\n",
    "Evaluate the model's performance on the test set to understand its accuracy and other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cfc2602-4912-4c0d-9548-cd875d57a906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.3046218454837799, 'eval_runtime': 43.3839, 'eval_samples_per_second': 576.25, 'eval_steps_per_second': 72.031, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"Evaluation results:\", eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385187d0-26fb-4e0f-842d-406b428d97d5",
   "metadata": {},
   "source": [
    "## Hands-On Exercise: Fine-Tune Another Model\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "1. Choose a different pre-trained model from the Huggingface Hub (e.g., \"distilbert-base-uncased\").\n",
    "2. Load and tokenize the dataset.\n",
    "3. Configure training arguments.\n",
    "4. Fine-tune the model.\n",
    "5. Evaluate its performance.\n",
    "\n",
    "**Questions to Consider:**\n",
    "\n",
    "- How does the model's performance compare to BERT?\n",
    "- What are the differences in memory usage between models?\n",
    "- What steps can be taken to optimize memory usage further?\n",
    "\n",
    "Try it out below!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ede54b82-9412-438b-991c-cf870df76798",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /leonardo_scratch/fast/EUHPC_D20_063/huggingface/models/distilbert--distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2407bb68a7e44f118577f8b836814305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dbc47ff55304ef68d9e9a38031170e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "596b43bdabee49b4b8b17f016a52b34a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 01:49, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.280200</td>\n",
       "      <td>0.325185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results for the new model: {'eval_loss': 0.3251846134662628, 'eval_runtime': 23.185, 'eval_samples_per_second': 1078.283, 'eval_steps_per_second': 134.785, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Choose a different model\n",
    "model_dir = \"/leonardo_scratch/fast/EUHPC_D20_063/huggingface/models/distilbert--distilbert-base-uncased\"  # Change to any other pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_dir, num_labels=2)\n",
    "\n",
    "# Step 2: Tokenize the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Step 3: Configure training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Step 4: Fine-tune the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test']\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"Evaluation results for the new model:\", eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539397e2-bdb7-4423-b8e8-12429e450022",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this hands-on example, you applied the concepts learned in previous sessions to fine-tune a pre-trained Transformer model on a text classification task. You practiced loading datasets, tokenizing text, and configuring training arguments to achieve optimal results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f182e3e4-8a8a-41ab-952f-9dead82ac27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': False}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shut down the kernel to release memory\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(restart=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945a374b-e8f4-4a4e-9122-ad2376a2d877",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
