{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0b1f163-108f-4587-8b0d-0246158ee528",
   "metadata": {},
   "source": [
    "# DDP example with Llama 3.2 1B Instruct and openassistant-guanaco dataset\n",
    "In this example a network is trained on multiple GPUs with the help of DDP (Distributed Data Parallel). This approach allows to train networks that fit into the memory of a single GPU on multiple GPUs in parallel in order to speed up the training.\n",
    "\n",
    "If we want to use multiple GPUs, we need to write the code to a file and submit the job to the SLURM scheduler, because the JupyterHub that we are using today does not have access to any GPU. This example uses two GPUs on one node, but could be extended simply by adjusting the number of GPUs and nodes in the SLURM script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d9ee84-e29b-4c05-b124-50e735033760",
   "metadata": {},
   "source": [
    "#### First, we write the python code to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "565c4533-5104-4a7c-a688-8b6acb72e17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting llama_guanaco_ddp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile llama_guanaco_ddp.py\n",
    "# Import libraries\n",
    "import torch\n",
    "from accelerate import PartialState\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import pynvml\n",
    "import psutil\n",
    "\n",
    "def set_cpu_affinity(local_rank):\n",
    "    # Leonardo has two NUMA nodes, CPUs 0-15 and 16-31.\n",
    "    # All four GPUs are connected to the first NUMA node.\n",
    "    # To find out which GPU belongs to which NUMA node, use the following command:\n",
    "    # `nvidia-smi topo -mp`\n",
    "    Leonardo_GPU_CPU_map = {\n",
    "        0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15],\n",
    "        1: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15],\n",
    "        2: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15],\n",
    "        3: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15],\n",
    "    }\n",
    "    cpu_list = Leonardo_GPU_CPU_map[local_rank]\n",
    "    print(f\"Local rank {local_rank} binding to cpus: {cpu_list}\")\n",
    "    psutil.Process().cpu_affinity(cpu_list)\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    pynvml.nvmlInit()\n",
    "    device_count = pynvml.nvmlDeviceGetCount()\n",
    "    memory_used = []\n",
    "    for device_index in range(device_count):\n",
    "        device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)\n",
    "        device_info = pynvml.nvmlDeviceGetMemoryInfo(device_handle)\n",
    "        memory_used.append(device_info.used/1024**3)\n",
    "    print('Memory occupied on GPUs: ' + ' + '.join([f'{mem:.1f}' for mem in memory_used]) + ' GB.')\n",
    "\n",
    "\n",
    "# Choose a model and load tokenizer and model (using 4bit quantization):\n",
    "# model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "model_name = \"/leonardo_scratch/fast/EUHPC_D20_063/huggingface/models/meta-llama--Llama-3.2-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# For some models (such as LLama-3.2-1B-Instruct), we need to set a padding token and the padding side:\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side='left'\n",
    "\n",
    "# For multi-GPU training, find out how many GPUs there are and which one we should use:\n",
    "ps = PartialState()\n",
    "num_processes = ps.num_processes\n",
    "process_index = ps.process_index\n",
    "local_process_index = ps.local_process_index\n",
    "set_cpu_affinity(local_process_index)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    ),\n",
    "    device_map={'':local_process_index},  # Changed for DDP\n",
    "    attn_implementation='eager',  # 'eager', 'sdpa', or \"flash_attention_2\"\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load the guanaco dataset\n",
    "guanaco_train = load_dataset('/leonardo_scratch/fast/EUHPC_D20_063/huggingface/datasets/timdettmers--openassistant-guanaco', split='train')\n",
    "guanaco_test = load_dataset('/leonardo_scratch/fast/EUHPC_D20_063/huggingface/datasets/timdettmers--openassistant-guanaco', split='test')\n",
    "# guanaco_train = load_dataset('timdettmers/openassistant-guanaco', split='train')\n",
    "# guanaco_test = load_dataset('timdettmers/openassistant-guanaco', split='test')\n",
    "\n",
    "def reformat_text(text, include_answer=True):\n",
    "    question1 = text.split('###')[1].removeprefix(' Human: ')\n",
    "    answer1 = text.split('###')[2].removeprefix(' Assistant: ')\n",
    "    if include_answer:\n",
    "        messages = [\n",
    "            {'role': 'user', 'content': question1},\n",
    "            {'role': 'assistant', 'content': answer1}\n",
    "        ]\n",
    "    else:\n",
    "        messages = [\n",
    "            {'role': 'user', 'content': question1}\n",
    "        ]        \n",
    "    reformatted_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    return reformatted_text\n",
    "\n",
    "# Now, apply reformat_train(..) to both datasets:\n",
    "guanaco_train = guanaco_train.map(lambda entry: {\n",
    "    'reformatted_text': reformat_text(entry['text'])\n",
    "})\n",
    "guanaco_test = guanaco_test.map(lambda entry: {\n",
    "    'reformatted_text': reformat_text(entry['text'])\n",
    "})\n",
    "\n",
    "model.config.use_cache = False  # KV cache can only speed up inference, but we are doing training.\n",
    "\n",
    "# Add low-rank adapters (LORA) to the model:\n",
    "peft_config = LoraConfig(\n",
    "    task_type='CAUSAL_LM',\n",
    "    r=16,\n",
    "    lora_alpha=32,  # thumb rule: lora_alpha should be 2*r\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    target_modules='all-linear',\n",
    ")\n",
    "\n",
    "training_arguments = SFTConfig(\n",
    "    output_dir='output/llama-3.2-1b-instruct-guanaco-ddp',\n",
    "    per_device_train_batch_size=8//num_processes,  # Adjust per-device batch size for DDP\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True, # Gradient checkpointing improves memory efficiency, but slows down training,\n",
    "        # e.g. Mistral 7B with PEFT using bitsandbytes:\n",
    "        # - enabled: 11 GB GPU RAM and 8 samples/second\n",
    "        # - disabled: 40 GB GPU RAM and 12 samples/second\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False},  # Use newer implementation that will become the default.\n",
    "    ddp_find_unused_parameters=False,  # Set to False when using gradient checkpointing to suppress warning message.\n",
    "    log_level_replica='error',  # Disable warnings in all but the first process.\n",
    "    optim='adamw_torch',\n",
    "    learning_rate=2e-4,  # QLoRA suggestions: 2e-4 for 7B or 13B, 1e-4 for 33B or 65B\n",
    "    logging_strategy='no',\n",
    "    # logging_strategy='steps',  # 'no', 'epoch' or 'steps'\n",
    "    # logging_steps=10,\n",
    "    save_strategy='no',  # 'no', 'epoch' or 'steps'\n",
    "    # save_steps=2000,\n",
    "    # num_train_epochs=5,\n",
    "    max_steps=100,\n",
    "    bf16=True,  # mixed precision training\n",
    "    report_to='none',  # disable wandb\n",
    "    max_length=1024,\n",
    "    dataset_text_field='reformatted_text',\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    peft_config=peft_config,\n",
    "    args=training_arguments,\n",
    "    train_dataset=guanaco_train,\n",
    "    eval_dataset=guanaco_test,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "if process_index == 0:  # Only print in first process.\n",
    "    if hasattr(trainer.model, \"print_trainable_parameters\"):\n",
    "        trainer.model.print_trainable_parameters()\n",
    "\n",
    "eval_result = trainer.evaluate()\n",
    "if process_index == 0:\n",
    "    print(\"Evaluation on test dataset before finetuning:\")\n",
    "    print(eval_result)\n",
    "\n",
    "train_result = trainer.train()\n",
    "if process_index == 0:\n",
    "    print(\"Training result:\")\n",
    "    print(train_result)\n",
    "\n",
    "eval_result = trainer.evaluate()\n",
    "if process_index == 0:\n",
    "    print(\"Evaluation on test dataset after finetuning:\")\n",
    "    print(eval_result)\n",
    "\n",
    "# Print memory usage once per node:\n",
    "if local_process_index == 0:\n",
    "    print_gpu_utilization()\n",
    "\n",
    "# # Save model in first process only:\n",
    "# if process_index == 0:\n",
    "#     trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1769457-c82f-4954-89a5-7b3b47ed72cc",
   "metadata": {},
   "source": [
    "#### Next, we write a SLURM script (initially using 1 GPU only):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2d643dc-4e7f-4aad-a24a-4d80d8cb33c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run_llama_guanaco_1gpu.slurm\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_llama_guanaco_1gpu.slurm\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --partition=boost_usr_prod\n",
    "# #SBATCH --qos=boost_qos_dbg\n",
    "#SBATCH --account=EUHPC_D20_063\n",
    "#SBATCH --reservation=s_tra_ncc\n",
    "\n",
    "## Specify resources:\n",
    "## Leonardo Booster: 32 CPU cores and 4 GPUs per node => request 8 * number of GPUs CPU cores\n",
    "## Leonardo Booster: 512 GB in total => request approx. 120 GB * number of GPUs requested\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --gpus-per-task=1  # up to 4 on Leonardo\n",
    "#SBATCH --ntasks-per-node=1  # always 1\n",
    "#SBATCH --mem=120GB  # should be 120GB * gpus-per-task on Leonardo\n",
    "#SBATCH --cpus-per-task=8  # should be 8 * gpus-per-task on Leonardo\n",
    "\n",
    "#SBATCH --time=0:30:00\n",
    "\n",
    "# Include commands in output:\n",
    "set -x\n",
    "\n",
    "# Print current time and date:\n",
    "date\n",
    "\n",
    "# Print host name:\n",
    "hostname\n",
    "\n",
    "# List available GPUs:\n",
    "nvidia-smi\n",
    "\n",
    "# Construct command to run container:\n",
    "export CONTAINER=\"singularity run --nv --home=$HOME $SINGULARITY_CONTAINER\"\n",
    "\n",
    "# Run:\n",
    "time $CONTAINER python3 llama_guanaco_ddp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4786f174-8231-4e1e-ae39-bff66ffccddc",
   "metadata": {},
   "source": [
    "#### We can now submit the SLURM script and, once the job ran, look at the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e8cc6fe-ec18-4856-b99a-e1e2f4f5ca86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 19692892\n"
     ]
    }
   ],
   "source": [
    "!sbatch --job-name=$TRAINEE_USERNAME run_llama_guanaco_1gpu.slurm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7886c3e1-da04-49b9-bf9d-806083239ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "          19692892 boost_usr trainee0 mpfister  R       0:09      1 lrdn0541\n"
     ]
    }
   ],
   "source": [
    "!squeue --name=$TRAINEE_USERNAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fde657-6fdd-4629-b177-59dce5521e9e",
   "metadata": {},
   "source": [
    "Change the number in the command below to the JOBID of the batch job that you just submitted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34e5628f-b9cd-4d89-93c1-58cd811225e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ export 'SING=singularity run --nv --home=/leonardo/home/userexternal/mpfister/one-click-hpc-access-home-trainee02 /leonardo_work/EUHPC_D12_020/mpfister/martin37b_5_nofa_novllm.sif'\n",
      "+ SING='singularity run --nv --home=/leonardo/home/userexternal/mpfister/one-click-hpc-access-home-trainee02 /leonardo_work/EUHPC_D12_020/mpfister/martin37b_5_nofa_novllm.sif'\n",
      "+ date\n",
      "Fri Sep  5 17:58:52 CEST 2025\n",
      "+ hostname\n",
      "lrdn0541.leonardo.local\n",
      "+ nvidia-smi\n",
      "Fri Sep  5 17:58:52 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM-64GB           On  | 00000000:C8:00.0 Off |                    0 |\n",
      "| N/A   43C    P0              61W / 453W |      2MiB / 65536MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "+ singularity run --nv --home=/leonardo/home/userexternal/mpfister/one-click-hpc-access-home-trainee02 /leonardo_work/EUHPC_D12_020/mpfister/martin37b_5_nofa_novllm.sif python3 llama_guanaco_ddp.py\n",
      "\n",
      "==========\n",
      "== CUDA ==\n",
      "==========\n",
      "\n",
      "CUDA Version 12.6.3\n",
      "\n",
      "Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
      "\n",
      "This container image and its contents are governed by the NVIDIA Deep Learning Container License.\n",
      "By pulling and using the container, you accept the terms and conditions of this license:\n",
      "https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n",
      "\n",
      "A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.\n",
      "\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "average_tokens_across_devices is set to True but it is invalid when world size is1. Turn it to False automatically.\n",
      "Local rank 0 binding to cpus: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15]\n",
      "Tokenizing train dataset: 100%|██████████| 9846/9846 [00:13<00:00, 720.79 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 9846/9846 [00:00<00:00, 198105.71 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 518/518 [00:00<00:00, 1334.81 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 518/518 [00:00<00:00, 28343.22 examples/s]\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[2025-09-05 17:59:50,365] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-09-05 17:59:53,766] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "trainable params: 11,272,192 || all params: 1,247,086,592 || trainable%: 0.9039\n",
      "100%|██████████| 65/65 [00:13<00:00,  4.71it/s]\n",
      "Evaluation on test dataset before finetuning:\n",
      "{'eval_loss': 2.497361660003662, 'eval_model_preparation_time': 0.0028, 'eval_runtime': 15.8834, 'eval_samples_per_second': 32.613, 'eval_steps_per_second': 4.092}\n",
      "100%|██████████| 100/100 [01:11<00:00,  1.40it/s]\n",
      "{'train_runtime': 71.4519, 'train_samples_per_second': 11.196, 'train_steps_per_second': 1.4, 'train_loss': 1.6018226623535157, 'num_tokens': 241179.0, 'mean_token_accuracy': 0.6527939939498901, 'epoch': 0.08}\n",
      "Training result:\n",
      "TrainOutput(global_step=100, training_loss=1.6018226623535157, metrics={'train_runtime': 71.4519, 'train_samples_per_second': 11.196, 'train_steps_per_second': 1.4, 'total_flos': 2938323255164928.0, 'train_loss': 1.6018226623535157})\n",
      "100%|██████████| 65/65 [00:13<00:00,  4.73it/s]\n",
      "Evaluation on test dataset after finetuning:\n",
      "{'eval_loss': 1.5624431371688843, 'eval_model_preparation_time': 0.0028, 'eval_runtime': 14.0419, 'eval_samples_per_second': 36.89, 'eval_steps_per_second': 4.629}\n",
      "Memory occupied on GPUs: 61.7 GB.\n",
      "\n",
      "real\t2m45.551s\n",
      "user\t1m54.375s\n",
      "sys\t0m32.057s\n"
     ]
    }
   ],
   "source": [
    "!cat slurm-19692892.out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9519e17-ec88-4b56-ab1d-1fbc4cb15612",
   "metadata": {},
   "source": [
    "#### Now, we write another SLURM script where use `torchrun` to train on multiple GPUs using DDP and submit the script to the scheduler again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c012ab2-b6ad-4078-aa60-a37ada2c8012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_llama_guanaco_ddp.slurm\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_llama_guanaco_ddp.slurm\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --partition=boost_usr_prod\n",
    "# #SBATCH --qos=boost_qos_dbg\n",
    "#SBATCH --account=EUHPC_D20_063\n",
    "#SBATCH --reservation=s_tra_ncc\n",
    "\n",
    "## Specify resources:\n",
    "## Leonardo Booster: 32 CPU cores and 4 GPUs per node => request 8 * number of GPUs CPU cores\n",
    "## Leonardo Booster: 512 GB in total => request approx. 120 GB * number of GPUs requested\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --gpus-per-task=2  # up to 4 on Leonardo\n",
    "#SBATCH --ntasks-per-node=1  # always 1\n",
    "#SBATCH --mem=240GB  # should be 120GB * gpus-per-task on Leonardo\n",
    "#SBATCH --cpus-per-task=16  # should be 8 * gpus-per-task on Leonardo\n",
    "\n",
    "#SBATCH --time=0:30:00\n",
    "\n",
    "# Include commands in output:\n",
    "set -x\n",
    "\n",
    "# Print current time and date:\n",
    "date\n",
    "\n",
    "# Print host name:\n",
    "hostname\n",
    "\n",
    "# List available GPUs:\n",
    "nvidia-smi\n",
    "\n",
    "# Construct command to run container:\n",
    "export CONTAINER=\"singularity run --nv --home=$HOME $SINGULARITY_CONTAINER\"\n",
    "\n",
    "# Set environment variables for communication between nodes:\n",
    "export MASTER_PORT=$(shuf -i 20000-30000 -n 1)  # Choose a random port\n",
    "export MASTER_ADDR=$(scontrol show hostnames ${SLURM_JOB_NODELIST} | head -n 1)\n",
    "export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n",
    "\n",
    "# Set launcher and launcher arguments:\n",
    "export LAUNCHER=\"torchrun \\\n",
    "    --nnodes=$SLURM_JOB_NUM_NODES \\\n",
    "    --nproc_per_node=$SLURM_GPUS_ON_NODE \\\n",
    "    --rdzv_id=$SLURM_JOB_ID \\\n",
    "    --rdzv_endpoint=$MASTER_ADDR:$MASTER_PORT \\\n",
    "    --rdzv_backend=c10d\"\n",
    "# Set training script that will be executed:\n",
    "export PROGRAM=\"llama_guanaco_ddp.py\"\n",
    "\n",
    "# Run:\n",
    "time srun bash -c \"$CONTAINER $LAUNCHER $PROGRAM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7a766c6-f7ec-49f6-a557-aa0b5d8ec357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 19693059\n"
     ]
    }
   ],
   "source": [
    "!sbatch --job-name=$TRAINEE_USERNAME run_llama_guanaco_ddp.slurm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37fd35dd-3ba0-4627-a53c-ca6542de7b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "          19693059 boost_usr trainee0 mpfister  R       0:05      1 lrdn2913\n"
     ]
    }
   ],
   "source": [
    "!squeue --name=$TRAINEE_USERNAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2e2a4c2-e157-40cd-9bc6-8de53b7065e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ date\n",
      "Fri Sep  5 18:05:17 CEST 2025\n",
      "+ hostname\n",
      "lrdn2913.leonardo.local\n",
      "+ nvidia-smi\n",
      "Fri Sep  5 18:05:18 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM-64GB           On  | 00000000:56:00.0 Off |                    0 |\n",
      "| N/A   44C    P0              68W / 483W |      2MiB / 65536MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM-64GB           On  | 00000000:8F:00.0 Off |                    0 |\n",
      "| N/A   43C    P0              64W / 458W |      2MiB / 65536MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "+ export 'CONTAINER=singularity run --nv --home=/leonardo/home/userexternal/mpfister/one-click-hpc-access-home-trainee02 /leonardo_work/EUHPC_D12_020/mpfister/martin37b_5_nofa_novllm.sif'\n",
      "+ CONTAINER='singularity run --nv --home=/leonardo/home/userexternal/mpfister/one-click-hpc-access-home-trainee02 /leonardo_work/EUHPC_D12_020/mpfister/martin37b_5_nofa_novllm.sif'\n",
      "++ shuf -i 20000-30000 -n 1\n",
      "+ export MASTER_PORT=21491\n",
      "+ MASTER_PORT=21491\n",
      "++ head -n 1\n",
      "++ scontrol show hostnames lrdn2913\n",
      "+ export MASTER_ADDR=lrdn2913\n",
      "+ MASTER_ADDR=lrdn2913\n",
      "+ export OMP_NUM_THREADS=16\n",
      "+ OMP_NUM_THREADS=16\n",
      "+ export 'LAUNCHER=torchrun     --nnodes=1     --nproc_per_node=2     --rdzv_id=19693059     --rdzv_endpoint=lrdn2913:21491     --rdzv_backend=c10d'\n",
      "+ LAUNCHER='torchrun     --nnodes=1     --nproc_per_node=2     --rdzv_id=19693059     --rdzv_endpoint=lrdn2913:21491     --rdzv_backend=c10d'\n",
      "+ export PROGRAM=llama_guanaco_ddp.py\n",
      "+ PROGRAM=llama_guanaco_ddp.py\n",
      "+ srun bash -c 'singularity run --nv --home=/leonardo/home/userexternal/mpfister/one-click-hpc-access-home-trainee02 /leonardo_work/EUHPC_D12_020/mpfister/martin37b_5_nofa_novllm.sif torchrun     --nnodes=1     --nproc_per_node=2     --rdzv_id=19693059     --rdzv_endpoint=lrdn2913:21491     --rdzv_backend=c10d llama_guanaco_ddp.py'\n",
      "\n",
      "==========\n",
      "== CUDA ==\n",
      "==========\n",
      "\n",
      "CUDA Version 12.6.3\n",
      "\n",
      "Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
      "\n",
      "This container image and its contents are governed by the NVIDIA Deep Learning Container License.\n",
      "By pulling and using the container, you accept the terms and conditions of this license:\n",
      "https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n",
      "\n",
      "A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.\n",
      "\n",
      "Local rank 1 binding to cpus: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15]\n",
      "Local rank 0 binding to cpus: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15]\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "/opt/project/.venv/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "[rank1]:[W905 18:05:56.310585539 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.\n",
      "/opt/project/.venv/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "[rank0]:[W905 18:05:56.435317086 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[2025-09-05 18:05:57,700] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-09-05 18:05:57,776] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-09-05 18:06:00,397] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
      "[2025-09-05 18:06:00,398] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "trainable params: 11,272,192 || all params: 1,247,086,592 || trainable%: 0.9039\n",
      "100%|██████████| 33/33 [00:08<00:00,  3.92it/s]\n",
      "Evaluation on test dataset before finetuning:\n",
      "{'eval_loss': 2.4874348640441895, 'eval_model_preparation_time': 0.0026, 'eval_runtime': 10.0611, 'eval_samples_per_second': 51.486, 'eval_steps_per_second': 3.28}\n",
      " 87%|████████▋ | 87/100 [00:34<00:05,  2.23it/{'train_runtime': 39.5605, 'train_samples_per_second': 20.222, 'train_steps_per_second': 2.528, 'train_loss': 1.5922088623046875, 'num_tokens': 241179.0, 'mean_token_accuracy': 0.652304254770279, 'epoch': 0.08}\n",
      "100%|██████████| 100/100 [00:39<00:00,  2.53it/s]\n",
      "Training result:\n",
      "TrainOutput(global_step=100, training_loss=1.5922088623046875, metrics={'train_runtime': 39.5605, 'train_samples_per_second': 20.222, 'train_steps_per_second': 2.528, 'total_flos': 2437947454390272.0, 'train_loss': 1.5922088623046875})\n",
      "100%|██████████| 33/33 [00:08<00:00,  3.95it/s]\n",
      "Evaluation on test dataset after finetuning:\n",
      "{'eval_loss': 1.568776249885559, 'eval_model_preparation_time': 0.0026, 'eval_runtime': 8.6714, 'eval_samples_per_second': 59.736, 'eval_steps_per_second': 3.806}\n",
      "Memory occupied on GPUs: 51.0 + 50.7 GB.\n",
      "\n",
      "real\t1m43.994s\n",
      "user\t0m0.269s\n",
      "sys\t0m0.010s\n"
     ]
    }
   ],
   "source": [
    "!cat slurm-19693059.out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e102f7d9-ca9f-486e-937e-c3ee3a09fc40",
   "metadata": {},
   "source": [
    "#### Finally, we can clean up and delete the files that we just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a52fe7c-9bfe-45d0-84ed-9287c9c84f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm llama_guanaco_ddp.py run_llama_guanaco_1gpu.slurm run_llama_guanaco_ddp.slurm slurm-*.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15b7d0b-924a-46f1-a23f-42ee5964dfec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fdaa48-f908-4b04-9132-681abc3a7ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a77a1b6-4241-48f1-8cfe-85b202e3d7c0",
   "metadata": {},
   "source": [
    "### Summary\n",
    "DDP allows to speed up training through the use of multiple GPUs for models that fit the memory of a single GPU.\n",
    "\n",
    "| Number of GPUs used | Training time |\n",
    "| - | - |\n",
    "| 1 GPU | 71 s |\n",
    "| 2 GPUs | 40 s |\n",
    "| 4 GPUs | ? |\n",
    "| 8 GPUs (2 nodes) | ? |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ec53ea-9e08-48d0-9b90-b3212543914d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
