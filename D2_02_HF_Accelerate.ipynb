{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fb8d0c9-ab40-455a-b6d8-9478c9ffb2e0",
   "metadata": {},
   "source": [
    "# Huggingface Accelerate\n",
    "\n",
    "In this notebook, we are going to write a Python and SLURM script directly from within cells and also launch the SLURM script.\n",
    "Since we only have limited ressources, we are going to use a small dateset in combination with a small model. However, this still demonstrates how to use Huggingface's [Accelerate](https://huggingface.co/docs/accelerate/index) library to perform distributed training on multiple GPUs across multiple nodes. In our case, we are using 2 nodes with 2 NVIDIA A100 GPUs each.\n",
    "The network used on LEONARDO is NVLink.\n",
    "\n",
    "Hugging Face Accelerate simplifies distributed training with the key benefits being:\n",
    "\n",
    " - Automated Distributed Setup: Accelerate automatically initializes the distributed environment. It configures process groups, sets the appropriate environment variables, and assigns GPUs to processes using PyTorch's native Distributed Data Parallel (DDP). This means you don’t have to manually set up multi-GPU execution or write boilerplate code.\n",
    "\n",
    " - Device and Process Management: With utilities like PartialState, Accelerate provides easy access to details such as the number of processes, process indices, and local device assignments. This information is crucial for tasks like sharding data, adjusting batch sizes per GPU, and ensuring that only one process handles logging or model saving.\n",
    "\n",
    " - Seamless Scaling: Accelerate allows your code to run on both single and multiple GPUs with minimal modifications. Whether you're training on one GPU or several, Accelerate handles the synchronization of model parameters and gradients across devices, making your code more portable and scalable.\n",
    "\n",
    "#### Python script\n",
    "Let's go through the Python script, so you know what we are launching here. <br>\n",
    "Accelerate doesn't mandate that the code be wrapped in a main() function, but it is highly recommended—especially for distributed or multi-GPU setups. Here’s why:\n",
    "\n",
    "- Multiprocessing Safety:\n",
    "    When using distributed training, processes are spawned that import your script. By wrapping your code in a main() function and using the if __name__ == \"__main__\": guard, you prevent unintended code execution in child processes.\n",
    "\n",
    "- Accelerate Configuration:\n",
    "The Accelerate config file often includes an entry like main_training_function: main. This instructs Accelerate to look for a function named main to kick off training. If you don’t define it, you might run into errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61c26ccb-8f0b-4b05-ae73-96739c07215f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting phi3_guanaco_accelerate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_llama_guanaco_accelerate.py\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import pynvml\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    pynvml.nvmlInit()\n",
    "    device_count = pynvml.nvmlDeviceGetCount()\n",
    "    memory_used = []\n",
    "    for device_index in range(device_count):\n",
    "        device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)\n",
    "        device_info = pynvml.nvmlDeviceGetMemoryInfo(device_handle)\n",
    "        memory_used.append(device_info.used / 1024**3)\n",
    "    print('Memory occupied on GPUs: ' + ' + '.join([f'{mem:.1f}' for mem in memory_used]) + ' GB.')\n",
    "\n",
    "def main():\n",
    "    # Initialize Accelerator; it will auto-detect the distributed environment from SLURM\n",
    "    accelerator = Accelerator()\n",
    "    device = accelerator.device\n",
    "    num_processes=accelerator.num_processes\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        print(f\"Running on device: {device}\")\n",
    "\n",
    "    # Define model name and load tokenizer\n",
    "    # model_name = '/leonardo_scratch/fast/EUHPC_D20_063/huggingface/models/microsoft--phi-3.5-mini-instruct'\n",
    "    model_name = '/leonardo_scratch/fast/EUHPC_D20_063/huggingface/models/meta-llama--Llama-3.2-1B-Instruct'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = 'left' # 'right'\n",
    "\n",
    "    # Load the model with 4-bit quantization. Note that we do not specify a device map manually.\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type='nf4',\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        ),\n",
    "        attn_implementation='eager',\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    # Move the model to the device specified by Accelerator\n",
    "    model.to(device)\n",
    "    \n",
    "    # Disable caching (only beneficial for inference)\n",
    "    model.config.use_cache = False\n",
    "\n",
    "\n",
    "    # Load the guanaco dataset\n",
    "    guanaco_train = load_dataset('/leonardo_scratch/fast/EUHPC_D20_063/huggingface/datasets/timdettmers--openassistant-guanaco', split='train')\n",
    "    guanaco_test = load_dataset('/leonardo_scratch/fast/EUHPC_D20_063/huggingface/datasets/timdettmers--openassistant-guanaco', split='test')\n",
    "    # guanaco_train = load_dataset('timdettmers/openassistant-guanaco', split='train')\n",
    "    # guanaco_test = load_dataset('timdettmers/openassistant-guanaco', split='test')\n",
    "\n",
    "    def reformat_text(text, include_answer=True):\n",
    "        question1 = text.split('###')[1].removeprefix(' Human: ')\n",
    "        answer1 = text.split('###')[2].removeprefix(' Assistant: ')\n",
    "        if include_answer:\n",
    "            messages = [\n",
    "                {'role': 'user', 'content': question1},\n",
    "                {'role': 'assistant', 'content': answer1}\n",
    "            ]\n",
    "        else:\n",
    "            messages = [\n",
    "                {'role': 'user', 'content': question1}\n",
    "            ]        \n",
    "        reformatted_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        return reformatted_text\n",
    "\n",
    "    # Now, apply reformat_train(..) to both datasets:\n",
    "    guanaco_train = guanaco_train.map(lambda entry: {\n",
    "        'reformatted_text': reformat_text(entry['text'])\n",
    "    })\n",
    "    guanaco_test = guanaco_test.map(lambda entry: {\n",
    "        'reformatted_text': reformat_text(entry['text'])\n",
    "    })\n",
    "\n",
    "    model.config.use_cache = False  # KV cache can only speed up inference, but we are doing training.\n",
    "    \n",
    "    # Add low-rank adapters (LORA) to the model:\n",
    "    peft_config = LoraConfig(\n",
    "        task_type='CAUSAL_LM',\n",
    "        r=16,\n",
    "        lora_alpha=32,  # thumb rule: lora_alpha should be 2*r\n",
    "        lora_dropout=0.05,\n",
    "        bias='none',\n",
    "        target_modules='all-linear',\n",
    "    )\n",
    "\n",
    "\n",
    "    training_arguments = SFTConfig(\n",
    "        output_dir='output/llama-3.2-1b-instruct-guanaco-ddp',\n",
    "        per_device_train_batch_size=8//num_processes,  # Adjust per-device batch size for DDP\n",
    "        gradient_accumulation_steps=1,\n",
    "        gradient_checkpointing=True, # Gradient checkpointing improves memory efficiency, but slows down training,\n",
    "            # e.g. Mistral 7B with PEFT using bitsandbytes:\n",
    "            # - enabled: 11 GB GPU RAM and 8 samples/second\n",
    "            # - disabled: 40 GB GPU RAM and 12 samples/second\n",
    "        gradient_checkpointing_kwargs={'use_reentrant': False},  # Use newer implementation that will become the default.\n",
    "        ddp_find_unused_parameters=False,  # Set to False when using gradient checkpointing to suppress warning message.\n",
    "        log_level_replica='error',  # Disable warnings in all but the first process.\n",
    "        optim='adamw_torch',\n",
    "        learning_rate=2e-4,  # QLoRA suggestions: 2e-4 for 7B or 13B, 1e-4 for 33B or 65B\n",
    "        logging_strategy='no',\n",
    "        # logging_strategy='steps',  # 'no', 'epoch' or 'steps'\n",
    "        # logging_steps=10,\n",
    "        save_strategy='no',  # 'no', 'epoch' or 'steps'\n",
    "        # save_steps=2000,\n",
    "        # num_train_epochs=5,\n",
    "        max_steps=100,\n",
    "        bf16=True,  # mixed precision training\n",
    "        report_to='none',  # disable wandb\n",
    "        max_length=1024,\n",
    "        dataset_text_field='reformatted_text',\n",
    "    )\n",
    "\n",
    "\n",
    "    # Create the SFTTrainer.\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        peft_config=peft_config,\n",
    "        args=training_arguments,\n",
    "        train_dataset=guanaco_train,\n",
    "        eval_dataset=guanaco_test,\n",
    "        processing_class=tokenizer,\n",
    "    )\n",
    "\n",
    "    # Optionally print trainable parameters on the main process only.\n",
    "    if accelerator.is_main_process and hasattr(trainer.model, \"print_trainable_parameters\"):\n",
    "        trainer.model.print_trainable_parameters()\n",
    "\n",
    "    # Evaluate before training\n",
    "    eval_result = trainer.evaluate()\n",
    "    if accelerator.is_main_process:\n",
    "        print(\"Evaluation on test dataset before finetuning:\")\n",
    "        print(eval_result)\n",
    "\n",
    "    # Train the model\n",
    "    train_result = trainer.train()\n",
    "    if accelerator.is_main_process:\n",
    "        print(\"Training result:\")\n",
    "        print(train_result)\n",
    "\n",
    "    # Evaluate after training\n",
    "    eval_result = trainer.evaluate()\n",
    "    if accelerator.is_main_process:\n",
    "        print(\"Evaluation on test dataset after finetuning:\")\n",
    "        print(eval_result)\n",
    "\n",
    "    # Print GPU memory usage (only once per node)\n",
    "    if accelerator.local_process_index == 0:\n",
    "        print_gpu_utilization()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98a9e9e-74d8-49ca-b0c2-0fbf9c90318d",
   "metadata": {},
   "source": [
    "#### Next, we write a SLURM script, initially using 1 GPU only and the exact same setup as with the DDP example:\n",
    "While this code works and runs, it is handled differently, than when launching it with Accelerate. Note the times in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d106c10-5d66-4fa1-889c-2db3c01581d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run_phi3_guanaco_accelerate_1gpu.slurm\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_llama_guanaco_accelerate_1gpu.slurm\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --partition=boost_usr_prod\n",
    "# #SBATCH --qos=boost_qos_dbg\n",
    "#SBATCH --account=EUHPC_D20_063\n",
    "#SBATCH --reservation=s_tra_ncc\n",
    "\n",
    "## Specify resources:\n",
    "## Leonardo Booster: 32 CPU cores and 4 GPUs per node => request 8 * number of GPUs CPU cores\n",
    "## Leonardo Booster: 512 GB in total => request approx. 120 GB * number of GPUs requested\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --gpus-per-task=1  # up to 4 on Leonardo\n",
    "#SBATCH --ntasks-per-node=1  # always 1\n",
    "#SBATCH --mem=120GB  # should be 120GB * gpus-per-task on Leonardo\n",
    "#SBATCH --cpus-per-task=8  # should be 8 * gpus-per-task on Leonardo\n",
    "\n",
    "#SBATCH --time=0:30:00\n",
    "\n",
    "# Include commands in output:\n",
    "set -x\n",
    "\n",
    "# Print current time and date:\n",
    "date\n",
    "\n",
    "# Print host name:\n",
    "hostname\n",
    "\n",
    "# List available GPUs:\n",
    "nvidia-smi\n",
    "\n",
    "# Construct command to run container:\n",
    "export CONTAINER=\"singularity run --nv --home=$HOME $SINGULARITY_CONTAINER\"\n",
    "\n",
    "# Run:\n",
    "time $CONTAINER python3 run_llama_guanaco_accelerate.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a09f4af-dd63-4aa5-8df4-fa540d721ad3",
   "metadata": {},
   "source": [
    "#### We can now submit the SLURM script and, once the job ran, look at the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d42590c-ccbb-484f-8811-524a238e7d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 19819385\n"
     ]
    }
   ],
   "source": [
    "!sbatch run_llama_guanaco_accelerate_1gpu.slurm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21f3ff05-f7ba-4bee-be3a-00f3566c647b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "          19819102 boost_usr trainee0 sharriso  R      15:19      1 lrdn3456\n"
     ]
    }
   ],
   "source": [
    "!squeue --me"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26c5156-e8bf-405c-a360-ba6721e5b35e",
   "metadata": {},
   "source": [
    "Change the number in the command below to the JOBID of the batch job that you just submitted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ba8f461-306c-412c-a65b-4bd203c5e006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ date\n",
      "Wed Sep 10 21:49:35 CEST 2025\n",
      "+ hostname\n",
      "lrdn3394.leonardo.local\n",
      "+ nvidia-smi\n",
      "Wed Sep 10 21:49:35 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM-64GB           On  | 00000000:8F:00.0 Off |                    0 |\n",
      "| N/A   42C    P0              60W / 458W |      2MiB / 65536MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "+ export 'CONTAINER=singularity run --nv --home=/leonardo/home/userexternal/sharriso/one-click-hpc-access-home-trainee01 /leonardo/pub/userexternal/mpfister/martin37b_5_nofa_novllm.sif'\n",
      "+ CONTAINER='singularity run --nv --home=/leonardo/home/userexternal/sharriso/one-click-hpc-access-home-trainee01 /leonardo/pub/userexternal/mpfister/martin37b_5_nofa_novllm.sif'\n",
      "+ singularity run --nv --home=/leonardo/home/userexternal/sharriso/one-click-hpc-access-home-trainee01 /leonardo/pub/userexternal/mpfister/martin37b_5_nofa_novllm.sif python3 phi3_guanaco_accelerate.py\n",
      "\n",
      "==========\n",
      "== CUDA ==\n",
      "==========\n",
      "\n",
      "CUDA Version 12.6.3\n",
      "\n",
      "Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
      "\n",
      "This container image and its contents are governed by the NVIDIA Deep Learning Container License.\n",
      "By pulling and using the container, you accept the terms and conditions of this license:\n",
      "https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n",
      "\n",
      "A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.\n",
      "\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Running on device: cuda\n",
      "Map: 100%|██████████| 9846/9846 [00:00<00:00, 9916.28 examples/s] \n",
      "Map: 100%|██████████| 518/518 [00:00<00:00, 4965.87 examples/s]\n",
      "average_tokens_across_devices is set to True but it is invalid when world size is1. Turn it to False automatically.\n",
      "Adding EOS to train dataset: 100%|██████████| 9846/9846 [00:00<00:00, 28942.51 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 9846/9846 [00:05<00:00, 1658.03 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 9846/9846 [00:00<00:00, 180989.58 examples/s]\n",
      "Adding EOS to eval dataset: 100%|██████████| 518/518 [00:00<00:00, 14150.01 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 518/518 [00:00<00:00, 1629.95 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 518/518 [00:00<00:00, 48888.40 examples/s]\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[2025-09-10 21:50:08,259] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-09-10 21:50:10,934] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "trainable params: 11,272,192 || all params: 1,247,086,592 || trainable%: 0.9039\n",
      "100%|██████████| 65/65 [00:13<00:00,  4.75it/s]\n",
      "Evaluation on test dataset before finetuning:\n",
      "{'eval_loss': 2.4799916744232178, 'eval_model_preparation_time': 0.0027, 'eval_runtime': 14.7301, 'eval_samples_per_second': 35.166, 'eval_steps_per_second': 4.413}\n",
      "100%|██████████| 100/100 [01:10<00:00,  1.42it/s]\n",
      "{'train_runtime': 70.6186, 'train_samples_per_second': 11.328, 'train_steps_per_second': 1.416, 'train_loss': 1.6055380249023437, 'num_tokens': 241179.0, 'mean_token_accuracy': 0.6519788068532943, 'epoch': 0.08}\n",
      "Training result:\n",
      "TrainOutput(global_step=100, training_loss=1.6055380249023437, metrics={'train_runtime': 70.6186, 'train_samples_per_second': 11.328, 'train_steps_per_second': 1.416, 'total_flos': 2938323255164928.0, 'train_loss': 1.6055380249023437})\n",
      "100%|██████████| 65/65 [00:13<00:00,  4.77it/s]\n",
      "Evaluation on test dataset after finetuning:\n",
      "{'eval_loss': 1.5687658786773682, 'eval_model_preparation_time': 0.0027, 'eval_runtime': 13.9401, 'eval_samples_per_second': 37.159, 'eval_steps_per_second': 4.663}\n",
      "Memory occupied on GPUs: 61.7 GB.\n",
      "\n",
      "real\t2m17.964s\n",
      "user\t1m49.308s\n",
      "sys\t0m33.165s\n"
     ]
    }
   ],
   "source": [
    "!cat slurm-19819385.out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb511eed-6add-4df7-b6f5-09e14a1e6b28",
   "metadata": {},
   "source": [
    "#### Now, we will create an Accelerate config file and adapt the SLURM script, so that Accelerate launches the program. We are still only using 1 GPU:\n",
    "Note the times in the output again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "447a57d4-7c26-4833-b310-88b6b356bf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting accelerate_default_config_1gpu.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerate_default_config_1gpu.yaml\n",
    "compute_environment: LOCAL_MACHINE\n",
    "debug: false\n",
    "distributed_type: NO\n",
    "mixed_precision: bf16\n",
    "downcast_bf16: 'yes'\n",
    "machine_rank: 0\n",
    "main_training_function: main\n",
    "num_machines: 1\n",
    "num_processes: 1\n",
    "rdzv_backend: static\n",
    "same_network: true\n",
    "use_cpu: false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "367bc657-1aef-4ebc-8902-e6302c6759dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run_phi3_guanaco_accelerate_1gpu.slurm\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_llama_guanaco_accelerate_1gpu.slurm\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --partition=boost_usr_prod\n",
    "# #SBATCH --qos=boost_qos_dbg\n",
    "#SBATCH --account=EUHPC_D20_063\n",
    "#SBATCH --reservation=s_tra_ncc\n",
    "\n",
    "## Specify resources:\n",
    "## Leonardo Booster: 32 CPU cores and 4 GPUs per node => request 8 * number of GPUs CPU cores\n",
    "## Leonardo Booster: 512 GB in total => request approx. 120 GB * number of GPUs requested\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --gpus-per-task=1  # up to 4 on Leonardo\n",
    "#SBATCH --ntasks-per-node=1  # always 1\n",
    "#SBATCH --mem=120GB  # should be 120GB * gpus-per-task on Leonardo\n",
    "#SBATCH --cpus-per-task=8  # should be 8 * gpus-per-task on Leonardo\n",
    "\n",
    "#SBATCH --time=0:30:00\n",
    "\n",
    "# Include commands in output:\n",
    "set -x\n",
    "\n",
    "# Print current time and date:\n",
    "date\n",
    "\n",
    "# Print host name:\n",
    "hostname\n",
    "\n",
    "# List available GPUs:\n",
    "nvidia-smi\n",
    "\n",
    "# Set environment variables for communication between nodes:\n",
    "export MASTER_PORT=$(shuf -i 20000-30000 -n 1)  # Choose a random port\n",
    "export MASTER_ADDR=$(scontrol show hostnames ${SLURM_JOB_NODELIST} | head -n 1)\n",
    "export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n",
    "\n",
    "# Set launcher and launcher arguments:\n",
    "export LAUNCHER=\"accelerate launch \\\n",
    "    --num_machines $SLURM_NNODES \\\n",
    "    --num_processes $((SLURM_NNODES * SLURM_GPUS_ON_NODE)) \\\n",
    "    --num_cpu_threads_per_process 8 \\\n",
    "    --main_process_ip $MASTER_ADDR \\\n",
    "    --main_process_port $MASTER_PORT \\\n",
    "    --machine_rank \\$SLURM_PROCID \\\n",
    "    --config_file \\\"accelerate_default_config_1gpu.yaml\\\" \\\n",
    "    \"\n",
    "# Set training script that will be executed:\n",
    "export PROGRAM=\"run_llama_guanaco_accelerate.py\"\n",
    "\n",
    "# Construct command to run container:\n",
    "export CONTAINER=\"singularity run --nv --home=$HOME $SINGULARITY_CONTAINER\"\n",
    "\n",
    "# Run:\n",
    "time srun bash -c \"$CONTAINER $LAUNCHER $PROGRAM\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc1d528-e9d5-4a25-a32a-3c2136237d79",
   "metadata": {},
   "source": [
    "#### We can now execute the SLURM script and, once the job ran, look at the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "879b0935-a1be-4b40-835d-154a9ab1e537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 19819419\n"
     ]
    }
   ],
   "source": [
    "!sbatch run_llama_guanaco_accelerate_1gpu.slurm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b2084719-fd83-49bf-a7d6-5674b8e9fed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "          19819102 boost_usr trainee0 sharriso  R      18:22      1 lrdn3456\n"
     ]
    }
   ],
   "source": [
    "!squeue --me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ee77873-5768-4543-b4a1-2207156cc423",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ date\n",
      "Wed Sep 10 21:52:47 CEST 2025\n",
      "+ hostname\n",
      "lrdn2761.leonardo.local\n",
      "+ nvidia-smi\n",
      "Wed Sep 10 21:52:47 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM-64GB           On  | 00000000:1D:00.0 Off |                    0 |\n",
      "| N/A   43C    P0              68W / 488W |      2MiB / 65536MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "++ shuf -i 20000-30000 -n 1\n",
      "+ export MASTER_PORT=28149\n",
      "+ MASTER_PORT=28149\n",
      "++ scontrol show hostnames lrdn2761\n",
      "++ head -n 1\n",
      "+ export MASTER_ADDR=lrdn2761\n",
      "+ MASTER_ADDR=lrdn2761\n",
      "+ export OMP_NUM_THREADS=8\n",
      "+ OMP_NUM_THREADS=8\n",
      "+ export 'LAUNCHER=accelerate launch     --num_machines 1     --num_processes 1     --num_cpu_threads_per_process 8     --main_process_ip lrdn2761     --main_process_port 28149     --machine_rank $SLURM_PROCID     --config_file \"accelerate_default_config_1gpu.yaml\"     '\n",
      "+ LAUNCHER='accelerate launch     --num_machines 1     --num_processes 1     --num_cpu_threads_per_process 8     --main_process_ip lrdn2761     --main_process_port 28149     --machine_rank $SLURM_PROCID     --config_file \"accelerate_default_config_1gpu.yaml\"     '\n",
      "+ export PROGRAM=phi3_guanaco_accelerate.py\n",
      "+ PROGRAM=phi3_guanaco_accelerate.py\n",
      "+ export 'CONTAINER=singularity run --nv --home=/leonardo/home/userexternal/sharriso/one-click-hpc-access-home-trainee01 /leonardo/pub/userexternal/mpfister/martin37b_5_nofa_novllm.sif'\n",
      "+ CONTAINER='singularity run --nv --home=/leonardo/home/userexternal/sharriso/one-click-hpc-access-home-trainee01 /leonardo/pub/userexternal/mpfister/martin37b_5_nofa_novllm.sif'\n",
      "+ srun bash -c 'singularity run --nv --home=/leonardo/home/userexternal/sharriso/one-click-hpc-access-home-trainee01 /leonardo/pub/userexternal/mpfister/martin37b_5_nofa_novllm.sif accelerate launch     --num_machines 1     --num_processes 1     --num_cpu_threads_per_process 8     --main_process_ip lrdn2761     --main_process_port 28149     --machine_rank $SLURM_PROCID     --config_file \"accelerate_default_config_1gpu.yaml\"      phi3_guanaco_accelerate.py'\n",
      "\n",
      "==========\n",
      "== CUDA ==\n",
      "==========\n",
      "\n",
      "CUDA Version 12.6.3\n",
      "\n",
      "Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
      "\n",
      "This container image and its contents are governed by the NVIDIA Deep Learning Container License.\n",
      "By pulling and using the container, you accept the terms and conditions of this license:\n",
      "https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n",
      "\n",
      "A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.\n",
      "\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "average_tokens_across_devices is set to True but it is invalid when world size is1. Turn it to False automatically.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Running on device: cuda\n",
      "[2025-09-10 21:53:22,616] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-09-10 21:53:25,349] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "trainable params: 11,272,192 || all params: 1,247,086,592 || trainable%: 0.9039\n",
      "100%|██████████| 65/65 [00:13<00:00,  4.73it/s]\n",
      "Evaluation on test dataset before finetuning:\n",
      "{'eval_loss': 2.4799916744232178, 'eval_model_preparation_time': 0.0029, 'eval_runtime': 15.1496, 'eval_samples_per_second': 34.192, 'eval_steps_per_second': 4.291}\n",
      "100%|██████████| 100/100 [01:11<00:00,  1.40it/s]\n",
      "{'train_runtime': 71.377, 'train_samples_per_second': 11.208, 'train_steps_per_second': 1.401, 'train_loss': 1.6016500854492188, 'num_tokens': 241179.0, 'mean_token_accuracy': 0.6523687952756881, 'epoch': 0.08}\n",
      "Training result:\n",
      "TrainOutput(global_step=100, training_loss=1.6016500854492188, metrics={'train_runtime': 71.377, 'train_samples_per_second': 11.208, 'train_steps_per_second': 1.401, 'total_flos': 2938323255164928.0, 'train_loss': 1.6016500854492188})\n",
      "100%|██████████| 65/65 [00:13<00:00,  4.76it/s]\n",
      "Evaluation on test dataset after finetuning:\n",
      "{'eval_loss': 1.5628045797348022, 'eval_model_preparation_time': 0.0029, 'eval_runtime': 13.9534, 'eval_samples_per_second': 37.123, 'eval_steps_per_second': 4.658}\n",
      "Memory occupied on GPUs: 61.7 GB.\n",
      "\n",
      "real\t2m22.824s\n",
      "user\t0m0.280s\n",
      "sys\t0m0.007s\n"
     ]
    }
   ],
   "source": [
    "!cat slurm-19819419.out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f1e520-75c5-4c47-bedc-e586366d0fc8",
   "metadata": {},
   "source": [
    "#### Now, we write another config file and SLURM script to train on multiple GPUs using Accelerate and submit the script to the scheduler again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31027442-ae73-44ee-95f5-e1d0f0214e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting accelerate_default_config_multi_gpu.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerate_default_config_multi_gpu.yaml\n",
    "compute_environment: LOCAL_MACHINE\n",
    "debug: false\n",
    "distributed_type: MULTI_GPU\n",
    "mixed_precision: bf16\n",
    "downcast_bf16: 'yes'\n",
    "machine_rank: 0\n",
    "main_training_function: main\n",
    "num_machines: 2\n",
    "num_processes: 4\n",
    "rdzv_backend: static\n",
    "same_network: true\n",
    "use_cpu: false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "03807fe7-0a2c-470c-849c-7e0b887f06f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run_phi3_guanaco_accelerate_multigpu.slurm\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_llama_guanaco_accelerate_multigpu.slurm\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --partition=boost_usr_prod\n",
    "# #SBATCH --qos=boost_qos_dbg\n",
    "#SBATCH --account=EUHPC_D20_063\n",
    "#SBATCH --reservation=s_tra_ncc\n",
    "\n",
    "## Specify resources:\n",
    "## Leonardo Booster: 32 CPU cores and 4 GPUs per node => request 8 * number of GPUs CPU cores\n",
    "## Leonardo Booster: 512 GB in total => request approx. 120 GB * number of GPUs requested\n",
    "#SBATCH --nodes=2\n",
    "#SBATCH --gpus-per-task=2  # up to 4 on Leonardo\n",
    "#SBATCH --ntasks-per-node=1  # always 1\n",
    "#SBATCH --mem=120GB  # should be 120GB * gpus-per-task on Leonardo\n",
    "#SBATCH --cpus-per-task=16  # should be 8 * gpus-per-task on Leonardo\n",
    "\n",
    "#SBATCH --time=0:30:00\n",
    "\n",
    "# Include commands in output:\n",
    "set -x\n",
    "\n",
    "# Print current time and date:\n",
    "date\n",
    "\n",
    "# Print host name:\n",
    "hostname\n",
    "\n",
    "# List available GPUs:\n",
    "nvidia-smi\n",
    "\n",
    "# Set environment variables for communication between nodes:\n",
    "export MASTER_PORT=$(shuf -i 20000-30000 -n 1)  # Choose a random port\n",
    "export MASTER_ADDR=$(scontrol show hostnames ${SLURM_JOB_NODELIST} | head -n 1)\n",
    "export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n",
    "\n",
    "# Set launcher and launcher arguments:\n",
    "export LAUNCHER=\"accelerate launch \\\n",
    "    --num_machines $SLURM_NNODES \\\n",
    "    --num_processes $((SLURM_NNODES * SLURM_GPUS_ON_NODE)) \\\n",
    "    --num_cpu_threads_per_process 8 \\\n",
    "    --main_process_ip $MASTER_ADDR \\\n",
    "    --main_process_port $MASTER_PORT \\\n",
    "    --machine_rank \\$SLURM_PROCID \\\n",
    "    --config_file \\\"accelerate_default_config_multi_gpu.yaml\\\" \\\n",
    "    \"\n",
    "# Set training script that will be executed:\n",
    "export PROGRAM=\"run_llama_guanaco_accelerate.py\"\n",
    "\n",
    "# Construct command to run container:\n",
    "export CONTAINER=\"singularity run --nv --home=$HOME $SINGULARITY_CONTAINER\"\n",
    "\n",
    "# Run:\n",
    "time srun bash -c \"$CONTAINER $LAUNCHER $PROGRAM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7b77016-c158-4f30-b724-ba641947bb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 19819444\n"
     ]
    }
   ],
   "source": [
    "!sbatch run_llama_guanaco_accelerate_multigpu.slurm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0bf67e19-dc5b-4281-aa21-f4ac41dec56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "          19819444 boost_usr run_phi3 sharriso CG       1:33      2 lrdn[3397-3398]\n",
      "          19819102 boost_usr trainee0 sharriso  R      20:41      1 lrdn3456\n"
     ]
    }
   ],
   "source": [
    "!squeue --me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "52797ca8-b066-44f3-8e0f-a57f6b8a5504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ date\n",
      "Wed Sep 10 21:55:57 CEST 2025\n",
      "+ hostname\n",
      "lrdn3397.leonardo.local\n",
      "+ nvidia-smi\n",
      "Wed Sep 10 21:55:57 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM-64GB           On  | 00000000:1D:00.0 Off |                    0 |\n",
      "| N/A   42C    P0              64W / 483W |      2MiB / 65536MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM-64GB           On  | 00000000:56:00.0 Off |                    0 |\n",
      "| N/A   42C    P0              64W / 484W |      2MiB / 65536MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "++ shuf -i 20000-30000 -n 1\n",
      "+ export MASTER_PORT=24344\n",
      "+ MASTER_PORT=24344\n",
      "++ head -n 1\n",
      "++ scontrol show hostnames 'lrdn[3397-3398]'\n",
      "+ export MASTER_ADDR=lrdn3397\n",
      "+ MASTER_ADDR=lrdn3397\n",
      "+ export OMP_NUM_THREADS=16\n",
      "+ OMP_NUM_THREADS=16\n",
      "+ export 'LAUNCHER=accelerate launch     --num_machines 2     --num_processes 4     --num_cpu_threads_per_process 8     --main_process_ip lrdn3397     --main_process_port 24344     --machine_rank $SLURM_PROCID     --config_file \"accelerate_default_config_multi_gpu.yaml\"     '\n",
      "+ LAUNCHER='accelerate launch     --num_machines 2     --num_processes 4     --num_cpu_threads_per_process 8     --main_process_ip lrdn3397     --main_process_port 24344     --machine_rank $SLURM_PROCID     --config_file \"accelerate_default_config_multi_gpu.yaml\"     '\n",
      "+ export PROGRAM=phi3_guanaco_accelerate.py\n",
      "+ PROGRAM=phi3_guanaco_accelerate.py\n",
      "+ export 'CONTAINER=singularity run --nv --home=/leonardo/home/userexternal/sharriso/one-click-hpc-access-home-trainee01 /leonardo/pub/userexternal/mpfister/martin37b_5_nofa_novllm.sif'\n",
      "+ CONTAINER='singularity run --nv --home=/leonardo/home/userexternal/sharriso/one-click-hpc-access-home-trainee01 /leonardo/pub/userexternal/mpfister/martin37b_5_nofa_novllm.sif'\n",
      "+ srun bash -c 'singularity run --nv --home=/leonardo/home/userexternal/sharriso/one-click-hpc-access-home-trainee01 /leonardo/pub/userexternal/mpfister/martin37b_5_nofa_novllm.sif accelerate launch     --num_machines 2     --num_processes 4     --num_cpu_threads_per_process 8     --main_process_ip lrdn3397     --main_process_port 24344     --machine_rank $SLURM_PROCID     --config_file \"accelerate_default_config_multi_gpu.yaml\"      phi3_guanaco_accelerate.py'\n",
      "\n",
      "==========\n",
      "== CUDA ==\n",
      "==========\n",
      "\n",
      "CUDA Version 12.6.3\n",
      "\n",
      "Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
      "\n",
      "This container image and its contents are governed by the NVIDIA Deep Learning Container License.\n",
      "By pulling and using the container, you accept the terms and conditions of this license:\n",
      "https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n",
      "\n",
      "A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.\n",
      "\n",
      "\n",
      "==========\n",
      "== CUDA ==\n",
      "==========\n",
      "\n",
      "CUDA Version 12.6.3\n",
      "\n",
      "Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
      "\n",
      "This container image and its contents are governed by the NVIDIA Deep Learning Container License.\n",
      "By pulling and using the container, you accept the terms and conditions of this license:\n",
      "https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n",
      "\n",
      "A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.\n",
      "\n",
      "Running on device: cuda:0\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "/opt/project/.venv/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "[rank1]:[W910 21:56:39.055616099 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.\n",
      "/opt/project/.venv/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "[rank0]:[W910 21:56:39.177701255 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "/opt/project/.venv/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "[rank3]:[W910 21:56:41.979094904 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 3]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.\n",
      "/opt/project/.venv/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "[rank2]:[W910 21:56:41.985705530 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 2]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[2025-09-10 21:56:41,925] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-09-10 21:56:41,965] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-09-10 21:56:42,043] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-09-10 21:56:42,043] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-09-10 21:56:44,683] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
      "[2025-09-10 21:56:44,683] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "trainable params: 11,272,192 || all params: 1,247,086,592 || trainable%: 0.9039\n",
      "[2025-09-10 21:56:44,845] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
      "[2025-09-10 21:56:44,847] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "100%|██████████| 17/17 [00:05<00:00,  3.32it/s]\n",
      "Evaluation on test dataset before finetuning:\n",
      "{'eval_loss': 2.47019100189209, 'eval_model_preparation_time': 0.0028, 'eval_runtime': 6.756, 'eval_samples_per_second': 76.672, 'eval_steps_per_second': 2.516}\n",
      " 87%|████████▋ | 87/100 [00:23<00:04,  3.08it/{'train_runtime': 26.5467, 'train_samples_per_second': 30.136, 'train_steps_per_second': 3.767, 'train_loss': 1.5338626098632813, 'num_tokens': 241179.0, 'mean_token_accuracy': 0.6522820734977722, 'epoch': 0.08}\n",
      "100%|██████████| 100/100 [00:26<00:00,  3.79it/s]\n",
      "Training result:\n",
      "TrainOutput(global_step=100, training_loss=1.5338626098632813, metrics={'train_runtime': 26.5467, 'train_samples_per_second': 30.136, 'train_steps_per_second': 3.767, 'total_flos': 1907365582471168.0, 'train_loss': 1.5338626098632813})\n",
      "100%|██████████| 17/17 [00:05<00:00,  3.36it/s]\n",
      "Evaluation on test dataset after finetuning:\n",
      "Memory occupied on GPUs: 22.2 + 35.6 GB.\n",
      "{'eval_loss': 1.56283438205719, 'eval_model_preparation_time': 0.0028, 'eval_runtime': 5.4364, 'eval_samples_per_second': 95.283, 'eval_steps_per_second': 3.127}\n",
      "Memory occupied on GPUs: 29.3 + 49.9 GB.\n",
      "\n",
      "real\t1m30.821s\n",
      "user\t0m0.273s\n",
      "sys\t0m0.005s\n"
     ]
    }
   ],
   "source": [
    "!cat slurm-19819444.out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbfef9e-0d43-41d0-ac18-954d73e7075c",
   "metadata": {},
   "source": [
    "#### Before we close the notebook, we should clean up the files created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aab5d70f-70a6-4ee0-a080-c51bd0358dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm run_llama_guanaco_accelerate.py run_llama_guanaco_accelerate_1gpu.slurm run_llama_guanaco_accelerate_multigpu.slurm slurm-*.out *.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9452e84-1d1d-4669-a688-9cdaf25daf6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
