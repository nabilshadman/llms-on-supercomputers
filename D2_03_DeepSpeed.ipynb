{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa76d21e-5f67-4405-8de3-048673db1b71",
   "metadata": {},
   "source": [
    "# ZeRO with Deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e7333b1-a904-433c-a75a-d5966568adba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing phi3_guanaco_accelerate_deepspeed.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile phi3_guanaco_accelerate_deepspeed.py\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import pynvml\n",
    "import deepspeed\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    pynvml.nvmlInit()\n",
    "    device_count = pynvml.nvmlDeviceGetCount()\n",
    "    memory_used = []\n",
    "    for device_index in range(device_count):\n",
    "        device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)\n",
    "        device_info = pynvml.nvmlDeviceGetMemoryInfo(device_handle)\n",
    "        memory_used.append(device_info.used / 1024**3)\n",
    "    print('Memory occupied on GPUs: ' + ' + '.join([f'{mem:.1f}' for mem in memory_used]) + ' GB.')\n",
    "\n",
    "def main():\n",
    "    # Initialize Accelerator; its configuration (including DeepSpeed) is loaded from the config file.\n",
    "    accelerator = Accelerator()\n",
    "    device = accelerator.device\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        print(f\"Running on device: {device}\")\n",
    "\n",
    "    # Define model name and load tokenizer.\n",
    "    model_name = '/leonardo_scratch/fast/EUHPC_D20_063/huggingface/models/microsoft--phi-3.5-mini-instruct'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = 'right'\n",
    "\n",
    "    # Load the model with 4-bit quantization.\n",
    "    # Note: We no longer specify a manual device map because DeepSpeed (via. Accelerate) will handle device placement.\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type='nf4',\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_quant_storage=torch.bfloat16,\n",
    "        ),\n",
    "        attn_implementation='eager',\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "  \n",
    "    # For DeepSpeed integration, you can remove or comment out the following:\n",
    "    # model.to(device)\n",
    "    \n",
    "    # Disable caching (KV cache is only useful during inference).\n",
    "    #model.config.use_cache = False\n",
    "\n",
    "    # Add LoRA adapters.\n",
    "    peft_config = LoraConfig(\n",
    "        task_type='CAUSAL_LM',\n",
    "        r=16,\n",
    "        lora_alpha=32,       # rule of thumb: lora_alpha should be about 2 * r\n",
    "        lora_dropout=0.05,\n",
    "        bias='none',\n",
    "        target_modules='all-linear',\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    # Load and preprocess the dataset.\n",
    "    guanaco_train = load_dataset(\n",
    "        '/leonardo_scratch/fast/EUHPC_D20_063/huggingface/datasets/timdettmers--openassistant-guanaco', \n",
    "        split='train'\n",
    "    )\n",
    "    guanaco_test = load_dataset(\n",
    "        '/leonardo_scratch/fast/EUHPC_D20_063/huggingface/datasets/timdettmers--openassistant-guanaco', \n",
    "        split='test'\n",
    "    )\n",
    "    # Process each example to extract the user prompt and assistant response.\n",
    "    guanaco_train = guanaco_train.map(lambda entry: {\n",
    "        'question1': entry['text'].split('###')[1].removeprefix(' Human: '),\n",
    "        'answer1': entry['text'].split('###')[2].removeprefix(' Assistant: ')\n",
    "    })\n",
    "    guanaco_test = guanaco_test.map(lambda entry: {\n",
    "        'question1': entry['text'].split('###')[1].removeprefix(' Human: '),\n",
    "        'answer1': entry['text'].split('###')[2].removeprefix(' Assistant: ')\n",
    "    })\n",
    "    # Restructure to a chat format expected by our formatting function.\n",
    "    guanaco_train = guanaco_train.map(lambda entry: {'messages': [\n",
    "        {'role': 'user', 'content': entry['question1']},\n",
    "        {'role': 'assistant', 'content': entry['answer1']}\n",
    "    ]})\n",
    "    guanaco_test = guanaco_test.map(lambda entry: {'messages': [\n",
    "        {'role': 'user', 'content': entry['question1']},\n",
    "        {'role': 'assistant', 'content': entry['answer1']}\n",
    "    ]})\n",
    "\n",
    "    # Define training arguments.\n",
    "    # Here, we add the `deepspeed` parameter so that the Trainer will use DeepSpeed.\n",
    "    training_arguments = SFTConfig(\n",
    "        output_dir='output/phi-3.5-mini-instruct-guanaco-deepspeed',\n",
    "        #per_device_train_batch_size=8,\n",
    "        #gradient_accumulation_steps=1,\n",
    "        gradient_checkpointing=True,\n",
    "        #gradient_checkpointing_kwargs={'use_reentrant': False},\n",
    "        optim='adamw_torch',\n",
    "        learning_rate=2e-4, # QLoRA suggestions: 2e-4 for 7B or 13B, 1e-4 for 33B or 65B\n",
    "        logging_strategy='no',\n",
    "        save_strategy='no',\n",
    "        max_steps=100,\n",
    "        bf16=True,\n",
    "        report_to='none',\n",
    "        max_seq_length=1024,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    def formatting_func(entry):\n",
    "        return tokenizer.apply_chat_template(entry['messages'], tokenize=False)\n",
    "\n",
    "    # Create the SFTTrainer.\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_arguments,\n",
    "        train_dataset=guanaco_train,\n",
    "        eval_dataset=guanaco_test,\n",
    "        processing_class=tokenizer,\n",
    "        formatting_func=formatting_func,\n",
    "    )\n",
    "\n",
    "    # Optionally print trainable parameters on the main process only.\n",
    "    if accelerator.is_main_process and hasattr(trainer.model, \"print_trainable_parameters\"):\n",
    "        trainer.model.print_trainable_parameters()\n",
    "\n",
    "    # Evaluate before training.\n",
    "    eval_result = trainer.evaluate()\n",
    "    if accelerator.is_main_process:\n",
    "        print(\"Evaluation on test dataset before finetuning:\")\n",
    "        print(eval_result)\n",
    "\n",
    "    # Train the model.\n",
    "    train_result = trainer.train()\n",
    "    if accelerator.is_main_process:\n",
    "        print(\"Training result:\")\n",
    "        print(train_result)\n",
    "\n",
    "    # Evaluate after training.\n",
    "    eval_result = trainer.evaluate()\n",
    "    if accelerator.is_main_process:\n",
    "        print(\"Evaluation on test dataset after finetuning:\")\n",
    "        print(eval_result)\n",
    "\n",
    "    # Print GPU memory usage (only once per node).\n",
    "    if accelerator.local_process_index == 0:\n",
    "        print_gpu_utilization()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37973d74-77c0-48a5-bf5b-d66b816d18b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inline DeepSpeed configuration meant to roughly correspond to your FSDP settings\n",
    "# Use bf16 for mixed precision, similar to mixed_precision: bf16 in the FSDP config.\n",
    "# Optimizer settings (you can adjust these as needed, or remove if not required):\n",
    "# Use ZeRO optimization stage 3 to mimic FSDP's full sharding:\n",
    "# Do not offload parameters (fsdp_offload_params: false)\n",
    "# These options are chosen to be simple; they differ from FSDPâ€™s wrapping or prefetching policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08d90978-c5a0-4bc3-840c-d0c272474868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing accelerate_deepspeed_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerate_deepspeed_config.yaml\n",
    "compute_environment: LOCAL_MACHINE\n",
    "debug: false\n",
    "distributed_type: DEEPSPEED\n",
    "downcast_bf16: 'no'\n",
    "deepspeed_config:\n",
    "    bf16: true\n",
    "    zero_stage: 3\n",
    "machine_rank: 0\n",
    "main_training_function: main\n",
    "mixed_precision: bf16\n",
    "num_machines: 2\n",
    "num_processes: 4\n",
    "rdzv_backend: c10d\n",
    "same_network: true\n",
    "use_cpu: false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9cdaa92-0a89-46fc-9f99-fed07f005d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_phi3_guanaco_accelerate_deepspeed.slurm\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_phi3_guanaco_accelerate_deepspeed.slurm\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --partition=boost_usr_prod\n",
    "# #SBATCH --qos=boost_qos_dbg\n",
    "#SBATCH --account=EUHPC_D20_063\n",
    "#SBATCH --reservation=s_tra_ncc\n",
    "\n",
    "## Specify resources:\n",
    "## Leonardo Booster: 32 CPU cores and 4 GPUs per node => request 8 * number of GPUs CPU cores\n",
    "## Leonardo Booster: 512 GB in total => request approx. 120 GB * number of GPUs requested\n",
    "#SBATCH --nodes=2\n",
    "#SBATCH --gpus-per-task=2  # up to 4 on Leonardo\n",
    "#SBATCH --ntasks-per-node=1  # always 1\n",
    "#SBATCH --mem=120GB  # should be 120GB * gpus-per-task on Leonardo\n",
    "#SBATCH --cpus-per-task=16  # should be 8 * gpus-per-task on Leonardo\n",
    "\n",
    "#SBATCH --time=0:30:00\n",
    "\n",
    "# Load conda:\n",
    "module purge\n",
    "module load anaconda3\n",
    "eval \"$(conda shell.bash hook)\"\n",
    "conda activate /leonardo/pub/userexternal/mpfister/conda_env_martin24\n",
    "\n",
    "# Include commands in output:\n",
    "set -x\n",
    "\n",
    "# Print current time and date:\n",
    "date\n",
    "\n",
    "# Print host name:\n",
    "hostname\n",
    "\n",
    "# List available GPUs:\n",
    "nvidia-smi\n",
    "\n",
    "# Set environment variables for communication between nodes:\n",
    "export MASTER_PORT=$(shuf -i 20000-30000 -n 1)  # Choose a random port\n",
    "export MASTER_ADDR=$(scontrol show hostnames ${SLURM_JOB_NODELIST} | head -n 1)\n",
    "export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n",
    "\n",
    "# Set launcher and launcher arguments:\n",
    "export LAUNCHER=\"accelerate launch \\\n",
    "    --num_machines $SLURM_NNODES \\\n",
    "    --num_processes $((SLURM_NNODES * SLURM_GPUS_ON_NODE/2)) \\\n",
    "    --num_cpu_threads_per_process 8 \\\n",
    "    --main_process_ip $MASTER_ADDR \\\n",
    "    --main_process_port $MASTER_PORT \\\n",
    "    --machine_rank \\$SLURM_PROCID \\\n",
    "    --config_file \\\"./accelerate_deepspeed_config.yaml\\\" \\\n",
    "    \"\n",
    "# Set training script that will be executed:\n",
    "export PROGRAM=\"phi3_guanaco_accelerate_deepspeed.py\"\n",
    "\n",
    "# Run:\n",
    "time srun bash -c \"$LAUNCHER $PROGRAM\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5b1e49-ddc0-4af9-879b-4ec276215b61",
   "metadata": {},
   "source": [
    "#### We can now execute the SLURM script and, once the job ran, look at the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d57e8f1c-65da-4475-8025-2323f512e6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 19837961\n"
     ]
    }
   ],
   "source": [
    "!sbatch run_phi3_guanaco_accelerate_deepspeed.slurm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09922169-bea3-4439-8cd5-bfb84d0f17bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "          19826558 boost_usr trainee4 a08trb42  R    4:50:25      1 lrdn0851\n",
      "          19837961 boost_usr run_phi3 a08trb42  R       0:00      2 lrdn[3428,3434]\n"
     ]
    }
   ],
   "source": [
    "!squeue --me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0292ef6-43d2-4a7c-a391-02f9f431dcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Unable to locate a modulefile for 'anaconda3'\n",
      "/var/spool/slurmd/job19837961/slurm_script: line 22: conda: command not found\n",
      "/var/spool/slurmd/job19837961/slurm_script: line 23: conda: command not found\n",
      "+ date\n",
      "Thu Sep 11 13:08:52 CEST 2025\n",
      "+ hostname\n",
      "lrdn3428.leonardo.local\n",
      "+ nvidia-smi\n",
      "Thu Sep 11 13:08:52 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM-64GB           On  | 00000000:1D:00.0 Off |                    0 |\n",
      "| N/A   43C    P0              61W / 465W |      2MiB / 65536MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM-64GB           On  | 00000000:56:00.0 Off |                    0 |\n",
      "| N/A   43C    P0              63W / 465W |      2MiB / 65536MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "++ shuf -i 20000-30000 -n 1\n",
      "+ export MASTER_PORT=25298\n",
      "+ MASTER_PORT=25298\n",
      "++ head -n 1\n",
      "++ scontrol show hostnames 'lrdn[3428,3434]'\n",
      "+ export MASTER_ADDR=lrdn3428\n",
      "+ MASTER_ADDR=lrdn3428\n",
      "+ export OMP_NUM_THREADS=16\n",
      "+ OMP_NUM_THREADS=16\n",
      "+ export 'LAUNCHER=accelerate launch     --num_machines 2     --num_processes 2     --num_cpu_threads_per_process 8     --main_process_ip lrdn3428     --main_process_port 25298     --machine_rank $SLURM_PROCID     --config_file \"./accelerate_deepspeed_config.yaml\"     '\n",
      "+ LAUNCHER='accelerate launch     --num_machines 2     --num_processes 2     --num_cpu_threads_per_process 8     --main_process_ip lrdn3428     --main_process_port 25298     --machine_rank $SLURM_PROCID     --config_file \"./accelerate_deepspeed_config.yaml\"     '\n",
      "+ export PROGRAM=phi3_guanaco_accelerate_deepspeed.py\n",
      "+ PROGRAM=phi3_guanaco_accelerate_deepspeed.py\n",
      "+ srun bash -c 'accelerate launch     --num_machines 2     --num_processes 2     --num_cpu_threads_per_process 8     --main_process_ip lrdn3428     --main_process_port 25298     --machine_rank $SLURM_PROCID     --config_file \"./accelerate_deepspeed_config.yaml\"      phi3_guanaco_accelerate_deepspeed.py'\n",
      "/usr/bin/bash: accelerate: command not found\n",
      "srun: error: lrdn3428: task 0: Exited with exit code 127\n",
      "srun: error: lrdn3434: task 1: Exited with exit code 127\n",
      "/usr/bin/bash: accelerate: command not found\n",
      "\n",
      "real\t0m0.404s\n",
      "user\t0m0.271s\n",
      "sys\t0m0.005s\n"
     ]
    }
   ],
   "source": [
    "!cat slurm-19837961.out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a697b550-e30f-471c-879d-8376825f0f5a",
   "metadata": {},
   "source": [
    "#### Before we close the notebook, we should clean up the files created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f03d5607-4f18-4579-9488-bc202107671c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'deepspeed_config*.json': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm phi3_guanaco_accelerate_deepspeed.py run_phi3_guanaco_accelerate_deepspeed.slurm slurm-*.out accelerate_deepspeed_config*.yaml deepspeed_config*.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa6fdb0-1dd8-491a-8e8b-0ee2e3b86bab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
