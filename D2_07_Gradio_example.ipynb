{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc68bcd6-bca8-4376-b231-0c8c84c532e4",
   "metadata": {},
   "source": [
    "## Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96197c4-9bdf-4895-a209-f97a30660b66",
   "metadata": {},
   "source": [
    "[Gradio](https://www.gradio.app) can enable simple web interfaces to your software. In this example, we are using Gradio to get a simple chat interface to a large language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8e10fab-fc0b-41c0-a794-8f73ae823ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting gradio_example.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile gradio_example.py\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "import gradio as gr\n",
    "import os\n",
    "import random\n",
    "import socket\n",
    "import sys\n",
    "\n",
    "ip = socket.gethostbyname(socket.gethostname())\n",
    "hostname = socket.gethostname().split('.')[0]\n",
    "port = random.randint(10000, 50000)\n",
    "trainee_user = os.environ['TRAINEE_USERNAME']\n",
    "\n",
    "print('Open the following URL in your webbrowser:')\n",
    "print(f'https://hpctraining.org/{trainee_user}/proxy/absolute/{hostname}:{port}/')\n",
    "print('')\n",
    "sys.stdout.flush()\n",
    "\n",
    "# model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "model_name = \"/leonardo_scratch/fast/EUHPC_D20_063/huggingface/models/meta-llama--Llama-3.2-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map='cuda',\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    ")\n",
    "pipe = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "def get_answer(question, history=[]):\n",
    "    history.append(\n",
    "        {'role': 'user', 'content': question}\n",
    "    )\n",
    "    result = pipe(history, max_new_tokens=500, return_full_text=False)\n",
    "    return result[0]['generated_text'].strip()\n",
    "    # return question\n",
    "\n",
    "chat_interface = gr.ChatInterface(get_answer, type='messages')\n",
    "chat_interface.launch(share=False, server_name=ip, server_port=port, root_path=f'/{trainee_user}/proxy/absolute/{hostname}:{port}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc0e4a77-b509-4e9a-8b44-d735c8539ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run_gradio_example.slurm\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_gradio_example.slurm\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --partition=boost_usr_prod\n",
    "# #SBATCH --qos=boost_qos_dbg\n",
    "#SBATCH --account=EUHPC_D20_063\n",
    "#SBATCH --reservation=s_tra_ncc\n",
    "\n",
    "## Specify resources:\n",
    "## Leonardo Booster: 32 CPU cores and 4 GPUs per node => request 8 * number of GPUs CPU cores\n",
    "## Leonardo Booster: 512 GB in total => request approx. 120 GB * number of GPUs requested\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --gpus-per-task=1  # up to 4 on Leonardo\n",
    "#SBATCH --ntasks-per-node=1  # always 1\n",
    "#SBATCH --mem=120GB  # should be 120GB * gpus-per-task on Leonardo\n",
    "#SBATCH --cpus-per-task=8  # should be 8 * gpus-per-task on Leonardo\n",
    "\n",
    "#SBATCH --time=0:10:00\n",
    "\n",
    "# Include commands in output:\n",
    "set -x\n",
    "\n",
    "# Print current time and date:\n",
    "date\n",
    "\n",
    "# Print host name:\n",
    "hostname\n",
    "\n",
    "# List available GPUs:\n",
    "nvidia-smi\n",
    "\n",
    "# Construct command to run container:\n",
    "export CONTAINER=\"singularity run --nv --home=$HOME $SINGULARITY_CONTAINER\"\n",
    "\n",
    "# Run AI scripts:\n",
    "$CONTAINER python3 gradio_example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7362f8e6-6855-491d-86dd-46de21d4d753",
   "metadata": {},
   "source": [
    "Now submit the SLURM job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eaf7a89-88ee-45f7-9fd0-fb5820db25c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 19769276\n"
     ]
    }
   ],
   "source": [
    "!sbatch --job-name=$TRAINEE_USERNAME run_gradio_example.slurm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f4ddfe-44de-4b68-aab8-00f08a610e8c",
   "metadata": {},
   "source": [
    "Execute `squeue` to see, if your job is already running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e10a89e-425e-4570-993b-c595869183e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "          19769276 boost_usr   martin mpfister  R       0:04      1 lrdn1321\n"
     ]
    }
   ],
   "source": [
    "!squeue --name=$TRAINEE_USERNAME "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072291e5-fea9-4821-9958-2da0b0ada7b5",
   "metadata": {},
   "source": [
    "Once your job is running, look at the output of the job using the following command (replace the number with the JOBID from above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d325f17-2e5c-4b26-82d4-98e5b4f52d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ date\n",
      "Tue Sep  9 15:06:17 CEST 2025\n",
      "+ hostname\n",
      "lrdn1321.leonardo.local\n",
      "+ nvidia-smi\n",
      "Tue Sep  9 15:06:17 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM-64GB           On  | 00000000:56:00.0 Off |                    0 |\n",
      "| N/A   43C    P0              60W / 458W |      2MiB / 65536MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "+ export 'CONTAINER=singularity run --nv --home=/leonardo/home/userexternal/mpfister/one-click-hpc-access-home-martin /leonardo/pub/userexternal/mpfister/martin37b_5_nofa_novllm.sif'\n",
      "+ CONTAINER='singularity run --nv --home=/leonardo/home/userexternal/mpfister/one-click-hpc-access-home-martin /leonardo/pub/userexternal/mpfister/martin37b_5_nofa_novllm.sif'\n",
      "+ singularity run --nv --home=/leonardo/home/userexternal/mpfister/one-click-hpc-access-home-martin /leonardo/pub/userexternal/mpfister/martin37b_5_nofa_novllm.sif python3 gradio_example.py\n",
      "\n",
      "==========\n",
      "== CUDA ==\n",
      "==========\n",
      "\n",
      "CUDA Version 12.6.3\n",
      "\n",
      "Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
      "\n",
      "This container image and its contents are governed by the NVIDIA Deep Learning Container License.\n",
      "By pulling and using the container, you accept the terms and conditions of this license:\n",
      "https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n",
      "\n",
      "A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.\n",
      "\n",
      "Open the following URL in your webbrowser:\n",
      "https://hpctraining.org/martin/proxy/absolute/lrdn1321:11061/\n",
      "\n",
      "Device set to use cuda\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "!cat slurm-19769276.out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aaff63-60eb-4cda-bbac-fb214b0a588a",
   "metadata": {},
   "source": [
    "Finally, when you are finished, please cancel the SLURM job to free the resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67bd34e4-fa5a-426d-b51b-218269740ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!scancel 19769276"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c020f6-e785-41f6-bb49-58d56994389c",
   "metadata": {},
   "source": [
    "If you want to, you can also delete the files that we create above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01cc3e8b-6303-46d4-9051-f3551e527e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm gradio_example.py run_gradio_example.slurm slurm-*.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b049395-b496-4412-abb7-1c5a7f592cce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
