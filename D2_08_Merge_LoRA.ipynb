{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc68bcd6-bca8-4376-b231-0c8c84c532e4",
   "metadata": {},
   "source": [
    "## Merge Low Rank Adapters to base model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96197c4-9bdf-4895-a209-f97a30660b66",
   "metadata": {},
   "source": [
    "This simple notebook demonstrates how to merge the Low Rank Adapters back into the base model and save the merged model in unquantized state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8e10fab-fc0b-41c0-a794-8f73ae823ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting merge_lora.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile merge_lora.py\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "trained_model_path = 'output/llama-3.2-1b-instruct-guanaco-fsdp'\n",
    "output_dir = trained_model_path + '_merged'\n",
    "\n",
    "config = PeftConfig.from_pretrained(trained_model_path)\n",
    "base_model_name = config.base_model_name_or_path\n",
    "base_model_name\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "print(f'base_model: {base_model.num_parameters()} parameters')\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(base_model, trained_model_path)\n",
    "\n",
    "print(f'peft_model: {peft_model.num_parameters()} parameters')\n",
    "\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "\n",
    "print(f'merged_model: {merged_model.num_parameters()}')\n",
    "\n",
    "merged_model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f'Quantized model saved to \"{output_dir}\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc0e4a77-b509-4e9a-8b44-d735c8539ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run_merge_lora.slurm\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_merge_lora.slurm\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --partition=boost_usr_prod\n",
    "# #SBATCH --qos=boost_qos_dbg\n",
    "#SBATCH --account=EUHPC_D20_063\n",
    "#SBATCH --reservation=s_tra_ncc\n",
    "\n",
    "## Specify resources:\n",
    "## Leonardo Booster: 32 CPU cores and 4 GPUs per node => request 8 * number of GPUs CPU cores\n",
    "## Leonardo Booster: 512 GB in total => request approx. 120 GB * number of GPUs requested\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --gpus-per-task=1  # up to 4 on Leonardo\n",
    "#SBATCH --ntasks-per-node=1  # always 1\n",
    "#SBATCH --mem=120GB  # should be 120GB * gpus-per-task on Leonardo\n",
    "#SBATCH --cpus-per-task=8  # should be 8 * gpus-per-task on Leonardo\n",
    "\n",
    "#SBATCH --time=0:10:00\n",
    "\n",
    "# Include commands in output:\n",
    "set -x\n",
    "\n",
    "# Print current time and date:\n",
    "date\n",
    "\n",
    "# Print host name:\n",
    "hostname\n",
    "\n",
    "# List available GPUs:\n",
    "nvidia-smi\n",
    "\n",
    "# Construct command to run container:\n",
    "export CONTAINER=\"singularity run --nv --home=$HOME $SINGULARITY_CONTAINER\"\n",
    "\n",
    "# Run AI scripts:\n",
    "$CONTAINER python3 merge_lora.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7362f8e6-6855-491d-86dd-46de21d4d753",
   "metadata": {},
   "source": [
    "Now submit the SLURM job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eaf7a89-88ee-45f7-9fd0-fb5820db25c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 19816821\n"
     ]
    }
   ],
   "source": [
    "!sbatch --job-name=$TRAINEE_USERNAME run_merge_lora.slurm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f4ddfe-44de-4b68-aab8-00f08a610e8c",
   "metadata": {},
   "source": [
    "Execute `squeue` to see, if your job is already running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e10a89e-425e-4570-993b-c595869183e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "          19816821 boost_usr   martin mpfister  R       0:04      1 lrdn3261\n"
     ]
    }
   ],
   "source": [
    "!squeue --name=$TRAINEE_USERNAME "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072291e5-fea9-4821-9958-2da0b0ada7b5",
   "metadata": {},
   "source": [
    "Once your job is running, look at the output of the job using the following command (replace the number with the JOBID from above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d325f17-2e5c-4b26-82d4-98e5b4f52d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ date\n",
      "Wed Sep 10 19:35:25 CEST 2025\n",
      "+ hostname\n",
      "lrdn3261.leonardo.local\n",
      "+ nvidia-smi\n",
      "Wed Sep 10 19:35:25 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM-64GB           On  | 00000000:8F:00.0 Off |                    0 |\n",
      "| N/A   42C    P0              59W / 456W |      2MiB / 65536MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "+ export 'CONTAINER=singularity run --nv --home=/leonardo/home/userexternal/mpfister/one-click-hpc-access-home-martin /leonardo/pub/userexternal/mpfister/martin37b_5_nofa_novllm.sif'\n",
      "+ CONTAINER='singularity run --nv --home=/leonardo/home/userexternal/mpfister/one-click-hpc-access-home-martin /leonardo/pub/userexternal/mpfister/martin37b_5_nofa_novllm.sif'\n",
      "+ singularity run --nv --home=/leonardo/home/userexternal/mpfister/one-click-hpc-access-home-martin /leonardo/pub/userexternal/mpfister/martin37b_5_nofa_novllm.sif python3 merge_lora.py\n",
      "\n",
      "==========\n",
      "== CUDA ==\n",
      "==========\n",
      "\n",
      "CUDA Version 12.6.3\n",
      "\n",
      "Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
      "\n",
      "This container image and its contents are governed by the NVIDIA Deep Learning Container License.\n",
      "By pulling and using the container, you accept the terms and conditions of this license:\n",
      "https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n",
      "\n",
      "A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.\n",
      "\n",
      "base_model: 1235814400 parameters\n",
      "peft_model: 1247086592 parameters\n",
      "merged_model: 1235814400\n",
      "[2025-09-10 19:36:04,215] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-09-10 19:36:07,468] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
      "Quantized model saved to \"output/llama-3.2-1b-instruct-guanaco-fsdp_merged\".\n"
     ]
    }
   ],
   "source": [
    "!cat slurm-19816821.out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c020f6-e785-41f6-bb49-58d56994389c",
   "metadata": {},
   "source": [
    "If you want to, you can also delete the files that we create above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01cc3e8b-6303-46d4-9051-f3551e527e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm merge_lora.py run_merge_lora.slurm slurm-*.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b049395-b496-4412-abb7-1c5a7f592cce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
