{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc68bcd6-bca8-4376-b231-0c8c84c532e4",
   "metadata": {},
   "source": [
    "## Load a model with VLLM and serve an OpenAI-compatible API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96197c4-9bdf-4895-a209-f97a30660b66",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to load a model (we are using the bitsandbytes quantized merged model from the previous notebook) and serve it for inference using VLLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc0e4a77-b509-4e9a-8b44-d735c8539ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_vllm.slurm\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_vllm.slurm\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --partition=boost_usr_prod\n",
    "# #SBATCH --qos=boost_qos_dbg\n",
    "#SBATCH --account=EUHPC_D20_063\n",
    "#SBATCH --reservation=s_tra_ncc\n",
    "\n",
    "## Specify resources:\n",
    "## Leonardo Booster: 32 CPU cores and 4 GPUs per node => request 8 * number of GPUs CPU cores\n",
    "## Leonardo Booster: 512 GB in total => request approx. 120 GB * number of GPUs requested\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --gpus-per-task=1  # up to 4 on Leonardo\n",
    "#SBATCH --ntasks-per-node=1  # always 1\n",
    "#SBATCH --mem=120GB  # should be 120GB * gpus-per-task on Leonardo\n",
    "#SBATCH --cpus-per-task=8  # should be 8 * gpus-per-task on Leonardo\n",
    "\n",
    "#SBATCH --time=0:10:00\n",
    "\n",
    "# Construct command to run container:\n",
    "export CONTAINER=\"singularity run --nv --home=$HOME /leonardo/pub/userexternal/mpfister/vllm01.sif\"\n",
    "\n",
    "# Run AI scripts:\n",
    "export OMP_NUM_THREADS=32\n",
    "export PORT=$((RANDOM % (60000 - 8000) + 8000))  # Choose a random port\n",
    "echo \"VLLM will serve an OpenAI compatible AI at http://$HOSTNAME:$PORT/v1\"\n",
    "$CONTAINER python3 -m vllm.entrypoints.openai.api_server \\\n",
    "    --model output/llama-3.2-1b-instruct-guanaco-fsdp_merged_bnb \\\n",
    "    --served-model-name finetuned-phi-3.5-mini \\\n",
    "    --dtype bfloat16 \\\n",
    "    --max-model-len 4096 \\\n",
    "    --api-key our-secret-api-key \\\n",
    "    --port $PORT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7362f8e6-6855-491d-86dd-46de21d4d753",
   "metadata": {},
   "source": [
    "Now submit the SLURM job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eaf7a89-88ee-45f7-9fd0-fb5820db25c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 19816220\n"
     ]
    }
   ],
   "source": [
    "!sbatch --job-name=$TRAINEE_USERNAME run_vllm.slurm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f4ddfe-44de-4b68-aab8-00f08a610e8c",
   "metadata": {},
   "source": [
    "Execute `squeue` to see, if your job is already running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e10a89e-425e-4570-993b-c595869183e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "          19816220 boost_usr   martin mpfister  R       0:04      1 lrdn3394\n"
     ]
    }
   ],
   "source": [
    "!squeue --name=$TRAINEE_USERNAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072291e5-fea9-4821-9958-2da0b0ada7b5",
   "metadata": {},
   "source": [
    "Once your job is running, look at the output of the job using the following command (replace the number with the JOBID from above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d325f17-2e5c-4b26-82d4-98e5b4f52d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VLLM will serve an OpenAI compatible AI at http://lrdn3394:30278/v1\n",
      "\n",
      "==========\n",
      "== CUDA ==\n",
      "==========\n",
      "\n",
      "CUDA Version 12.6.3\n",
      "\n",
      "Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
      "\n",
      "This container image and its contents are governed by the NVIDIA Deep Learning Container License.\n",
      "By pulling and using the container, you accept the terms and conditions of this license:\n",
      "https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n",
      "\n",
      "A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.\n",
      "\n",
      "INFO 09-10 19:06:27 [__init__.py:241] Automatically detected platform cuda.\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:06:29 [api_server.py:1805] vLLM API server version 0.10.1.1\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:06:29 [utils.py:326] non-default args: {'port': 30278, 'api_key': ['our-secret-api-key'], 'model': 'output/llama-3.2-1b-instruct-guanaco-fsdp_merged_bnb', 'dtype': 'bfloat16', 'max_model_len': 4096, 'served_model_name': ['finetuned-phi-3.5-mini']}\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:06:40 [__init__.py:711] Resolved architecture: LlamaForCausalLM\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m `torch_dtype` is deprecated! Use `dtype` instead!\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:06:40 [__init__.py:1750] Using max model len 4096\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m WARNING 09-10 19:06:42 [__init__.py:1171] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:06:42 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 09-10 19:06:52 [__init__.py:241] Automatically detected platform cuda.\n",
      "\u001b[1;36m(EngineCore_0 pid=3008666)\u001b[0;0m INFO 09-10 19:06:55 [core.py:636] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_0 pid=3008666)\u001b[0;0m INFO 09-10 19:06:55 [core.py:74] Initializing a V1 LLM engine (v0.10.1.1) with config: model='output/llama-3.2-1b-instruct-guanaco-fsdp_merged_bnb', speculative_config=None, tokenizer='output/llama-3.2-1b-instruct-guanaco-fsdp_merged_bnb', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=finetuned-phi-3.5-mini, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_0 pid=3008666)\u001b[0;0m INFO 09-10 19:06:56 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_0 pid=3008666)\u001b[0;0m WARNING 09-10 19:06:56 [topk_topp_sampler.py:61] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_0 pid=3008666)\u001b[0;0m INFO 09-10 19:06:56 [gpu_model_runner.py:1953] Starting to load model output/llama-3.2-1b-instruct-guanaco-fsdp_merged_bnb...\n",
      "\u001b[1;36m(EngineCore_0 pid=3008666)\u001b[0;0m INFO 09-10 19:06:56 [gpu_model_runner.py:1985] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_0 pid=3008666)\u001b[0;0m INFO 09-10 19:06:57 [cuda.py:328] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_0 pid=3008666)\u001b[0;0m INFO 09-10 19:06:57 [bitsandbytes_loader.py:742] Loading weights with BitsAndBytes quantization. May take a while ...\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 85.54it/s]\n",
      "\u001b[1;36m(EngineCore_0 pid=3008666)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.61it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.61it/s]\n",
      "\u001b[1;36m(EngineCore_0 pid=3008666)\u001b[0;0m \n",
      "\u001b[1;36m(EngineCore_0 pid=3008666)\u001b[0;0m INFO 09-10 19:06:58 [gpu_model_runner.py:2007] Model loading took 1.0198 GiB and 0.902938 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=3008666)\u001b[0;0m INFO 09-10 19:07:03 [backends.py:548] Using cache directory: /leonardo/home/userexternal/mpfister/one-click-hpc-access-home-martin/.cache/vllm/torch_compile_cache/2ca282a95b/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_0 pid=3008666)\u001b[0;0m INFO 09-10 19:07:03 [backends.py:559] Dynamo bytecode transform time: 4.67 s\n",
      "\u001b[1;36m(EngineCore_0 pid=3008666)\u001b[0;0m INFO 09-10 19:07:06 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.591 s\n",
      "\u001b[1;36m(EngineCore_0 pid=3008666)\u001b[0;0m INFO 09-10 19:07:06 [monitor.py:34] torch.compile takes 4.67 s in total\n",
      "\u001b[1;36m(EngineCore_0 pid=3008666)\u001b[0;0m INFO 09-10 19:07:07 [gpu_worker.py:276] Available KV cache memory: 54.86 GiB\n",
      "\u001b[1;36m(EngineCore_0 pid=3008666)\u001b[0;0m INFO 09-10 19:07:07 [kv_cache_utils.py:849] GPU KV cache size: 1,797,696 tokens\n",
      "\u001b[1;36m(EngineCore_0 pid=3008666)\u001b[0;0m INFO 09-10 19:07:07 [kv_cache_utils.py:853] Maximum concurrency for 4,096 tokens per request: 438.89x\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:03<00:00, 19.18it/s]\n",
      "\u001b[1;36m(EngineCore_0 pid=3008666)\u001b[0;0m INFO 09-10 19:07:11 [gpu_model_runner.py:2708] Graph capturing finished in 4 secs, took 2.18 GiB\n",
      "\u001b[1;36m(EngineCore_0 pid=3008666)\u001b[0;0m INFO 09-10 19:07:11 [core.py:214] init engine (profile, create kv cache, warmup model) took 13.25 seconds\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [loggers.py:142] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 112356\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [api_server.py:1611] Supported_tasks: ['generate']\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m WARNING 09-10 19:07:12 [__init__.py:1625] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [serving_responses.py:120] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [serving_chat.py:134] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [serving_completion.py:77] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [api_server.py:1880] Starting vLLM API server 0 on http://0.0.0.0:30278\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [launcher.py:36] Available routes are:\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [launcher.py:44] Route: /openapi.json, Methods: GET, HEAD\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [launcher.py:44] Route: /docs, Methods: GET, HEAD\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [launcher.py:44] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [launcher.py:44] Route: /redoc, Methods: GET, HEAD\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [launcher.py:44] Route: /health, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [launcher.py:44] Route: /load, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [launcher.py:44] Route: /ping, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [launcher.py:44] Route: /ping, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [launcher.py:44] Route: /tokenize, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [launcher.py:44] Route: /detokenize, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [launcher.py:44] Route: /v1/models, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [launcher.py:44] Route: /version, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [launcher.py:44] Route: /v1/responses, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [launcher.py:44] Route: /v1/responses/{response_id}, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [launcher.py:44] Route: /v1/responses/{response_id}/cancel, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [launcher.py:44] Route: /v1/chat/completions, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [launcher.py:44] Route: /v1/completions, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [launcher.py:44] Route: /v1/embeddings, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [launcher.py:44] Route: /pooling, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [launcher.py:44] Route: /classify, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [launcher.py:44] Route: /score, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [launcher.py:44] Route: /v1/score, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [launcher.py:44] Route: /v1/audio/transcriptions, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [launcher.py:44] Route: /v1/audio/translations, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [launcher.py:44] Route: /rerank, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [launcher.py:44] Route: /v1/rerank, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [launcher.py:44] Route: /v2/rerank, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [launcher.py:44] Route: /scale_elastic_ep, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [launcher.py:44] Route: /is_scaling_elastic_ep, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [launcher.py:44] Route: /invocations, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO 09-10 19:07:12 [launcher.py:44] Route: /metrics, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO:     Started server process [3008577]\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO:     Waiting for application startup.\n",
      "\u001b[1;36m(APIServer pid=3008577)\u001b[0;0m INFO:     Application startup complete.\n",
      "slurmstepd: error: *** JOB 19816220 ON lrdn3394 CANCELLED AT 2025-09-10T19:19:31 DUE TO TIME LIMIT ***\n"
     ]
    }
   ],
   "source": [
    "!cat slurm-19816220.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3db6b1-d720-4029-abd7-ba7c214e8687",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48eab65a-9aa1-40d4-9ba9-c2744e492aa6",
   "metadata": {},
   "source": [
    "Now we can access the OpenAI-compatible API using any other software. For example, we can use the `openai` python library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f64792a-9a91-40f4-a115-a3675d0d8229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat response:\n",
      " ChatCompletion(id='chatcmpl-2bac2702524744de86c8e1612c8c66ff', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Why don't scientists trust atoms? Because they make up everything!\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None), stop_reason=None)], created=1757523921, model='finetuned-phi-3.5-mini', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=14, prompt_tokens=46, total_tokens=60, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None, kv_transfer_params=None)\n",
      "\n",
      "Answer from the chatbot: Why don't scientists trust atoms? Because they make up everything!\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "# Set OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"our-secret-api-key\"\n",
    "# TODO: Edit the following line to use the URL from the output of your own VLLM job\n",
    "openai_api_base = \"http://lrdn3394:30278/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=\"finetuned-phi-3.5-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a joke.\"},\n",
    "    ]\n",
    ")\n",
    "print('Chat response:\\n', chat_response)\n",
    "print('')\n",
    "print('Answer from the chatbot:', chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67826641-2cb6-45e9-b220-8a76eb1c7f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ec75488-c193-4b7c-8bba-166826672e7e",
   "metadata": {},
   "source": [
    "Finally, cancel your VLLM job to free up the resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3df32c4f-238c-46e1-9375-2582ea138c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "!scancel 19816220"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c020f6-e785-41f6-bb49-58d56994389c",
   "metadata": {},
   "source": [
    "If you want to, you can also delete the files that we create above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01cc3e8b-6303-46d4-9051-f3551e527e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm run_vllm.slurm slurm-*.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b049395-b496-4412-abb7-1c5a7f592cce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
